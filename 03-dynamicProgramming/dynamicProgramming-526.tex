\input{../noteHeader} 

\title{Dynamic Programming}
\date{\today}

\begin{document}

\maketitle

\section{References}

These notes are about dynamic programming.  References from our text
books are chapter 11 of \cite{dixit1990}. \cite{adda2003} is very nice
and available online from UBC library.  \cite{stokey1989} is the
classic economics reference for dynamic programming, but is more
advanced than what we will cover. 

\textit{Applied dynamic programming} by \cite{bellman1962} and
\textit{Dynamic programming and the calculus of variations} by Dreyfus
(1965) provide a good introduction to the main idea of dynamic
programming, and are especially useful for contrasting the dynamic
programming and optimal control approaches.  \cite{dreyfus2002} has
some amusing anecdotes from Bellman about the initial development of
dynamic programming.  \cite{bertsekas1976} is
the classic reference for dynamic programming with uncertainty.

\section{Introduction}

\begin{quotation}
  ``[Dynamic] also has a very interesting property as an adjective, and
  that is it’s impossible to use the word, dynamic, in a pejorative
  sense. Try thinking of some combination that will possibly give it a
  pejorative meaning.  It’s impossible. Thus, I thought dynamic
  programming was a good name.'' - Richard Bellman
\end{quotation}


\begin{example}\label{ex:consav}[Consumption-savings]
  An infinite horizon consumption-savings problem, 
  \[ \max_{\{c_t\}_{t=0}^\infty,\{s_t\}_{t=1}^\infty} \sum_{t=0}^\infty
  \beta^t u(c_t) \text{ s.t. } s_{t+1} = (1+r_t)(s_t - c_t), \]  
  involves maximizing over a countably infinite sequence of $c_t$ and
  $s_t$. The interpretation of this problem is that $u(c)$ is the
  per-period utility from consumption. $c_t$ is consumption at time
  $t$. $s_t$ is the savings you have at time $t$. $r_t$ is the return to
  savings at time $t$ in period $t+1$, and $\beta$ is the discount
  factor. 
\end{example}

For problems like examples \ref{ex:consav} and \ref{ex:contract},
optimal control focuses on characterizing $x^*$ through the first
order conditions (given $x^*$, $y^*$ is easily determined through the
constraint). That is, optimal control focus on characterizing the maximizer. An
alternative approach is to focus on the value of the maximized
function. This value will depend on the entire problem, but in
particular it depends on the initial condition $y_0$. Thus, we can
think of the value as function of the initial state. Dynamic
programming focuses on characterizing the value function.  

The basic idea of dynamic programming can be illustrated in a familiar
finite dimensional optimization problem. Consider a finite horizon
discrete time consumption savings choice. 
\[ \max_{c_t,s_t} \sum_{t=0}^T \beta^t u(c_t) \text{ s.t. } s_{t+1} =
(1+r_t) (s_t - c_t) \] with $s_0$ given, and the constraint that
$s_{T+1} = 0$. We could just write down the first order conditions and
try to solve them for $c_t$. However, if $T$ is large, this might be
very difficult. It can be especially difficult to calculate a solution
numerically. The easiest maximization problems to solve numerically
are ones where the objective function is linear or quadratic. In
either of these cases, the amount work needed is proportional to the
number of variables cubed. If $T$ is large, $T^3$ can be so large that
computing a solution takes prohibitively large. 

We can divide this $T$ dimensional problem to a
series of smaller ones by first thinking about what happens at time
$T$. At time $T$ we have some savings $s_T$ and want to choose $c_T$
to solve
\[ \max_{c_T} u(c_T) \text{ s.t. } s_{T+1} = (1+r_T)(s_T -c_T) = 0 \]
As long as $u$ is increasing, it must be that $c_T^*(s_T) = s_T$. If
we define the value of savings at time $T$ as
\[ V_T(s) = u(s), \]
then at time $T-1$ given $s_{T-1}$, we can choose $c_{T-1}$ to solve
\[ \max_{c_{T-1},s'} u(c_{T-1}) + \beta V_T(s') \text{ s.t.} s' =
  (1+r_{T-1})(c_{T-1}-s_{T-1}). \]
This is relatively simple maximization problem with just two
variables, so we can solve it without too much difficulty. Repeating
in this way, for each $t$ we can define the value of savings at time
$t$ as
\begin{align} 
  V_t(s) = \max_{c_t,s'} u(c_{t}) + \beta V_{t+1}(s') \text{ s.t.} s' =
  (1+r_{t})(c_{t}-s). \label{e:bell}
\end{align}
This approach to sequential optimization was first proposed by Richard
Bellman, so (\ref{e:bell}) is called a Bellman equation. Notice that
if $(c_t^*(s_t),s_{t+1}^*(s_t)$ is a maximizer of (\ref{e:bell}) for
each $t$, then 
the sequence of $c_0^*(s_0),s_1^*(s_0), c_1^*(s_1), ..., c_T^*$ must
be a maximizer of the original problem. Bellman called this
observation the \textbf{principle of optimality}. He described it as,
``An optimal policy has the property that whatever the initial state
and initial decision are, the remaining decisions must constitute an
optimal policy with regard to the state resulting from the first
decision.'' (\cite{bellman1962})

In finite horizon problems, it easy to see that the
Bellman equations will exist. However, if we have an infinite horizon
problem, 
\[ \max_{c_t,s_t} \sum_{t=0}^\infty \beta^t u(c_t) \text{ s.t. } s_{t+1} =
(1+r_t) (s_t - c_t) \]
then we cannot start from the last period to define the value
function. However, if the problem is stationary, that is if the
problem at time $t$ and at time $t+1$ are the same, then it seems
reasonable to think that the value function would not depend on $t$
and we could just write 
\[ V(s) = \max_{c,s'} u(c) + \beta V(s') \text{ s.t. } s' =
(1+r)(s-c). \]
Stokey and Lucas (1989) provide a fairly comprehensive analysis of
various conditions when this is possible. We will just look at one
case. 

Consider a problem that is slightly more general than the consumption
savings choice problem with fixed interest rate.
\begin{align*}
  \max_{c_t,s_t} & \sum_{t=0}^\infty \beta^t u(c_t,s_t) \\
  & s_{t+1} = g(c_t,s_t),
\end{align*}
where $c \in \R$, $s \in \R$, $0<\beta<1$, and $u,g:\R^2 \to \R$.
We want to show that the value function exists. Suppose we start with
some guess at the value function $v_0:\R\to \R$. Then we refine that guess by
setting 
\[ v_1(s) = \max_{c,s'} u(c,s) + \beta v_0(s') \text{ s.t. }
s'=g(c,s) \]
We could do this repeatedly. That is, if we let $T$ be the operator
defined by this equation,
\[ T(v)(s) = \max_{c,s'} u(c,s) + \beta v_0(s') \text{ s.t. }
s'=g(c,s), \]
we can construct a sequence with $v_{i+1} = T(v_i)$. 

We say that $T$ is a contraction mapping if 
\[ \norm{T(v) - T(v')} \leq c \norm{v-v'} \] for some $c<1$ and all
$v,v'$. We will prove that contraction mappings have unique fixed
points. Thus, if we can show that $T$ is a contraction mapping, then
$v_i \to V$, where $V$ is the value function that satisfies the
Bellman equation, like we want.

Suppose $u$ is bounded, $u(c,s) \leq M$ for all $c$ and $s$. Then 
\[ \sum_{t=0}^\infty \beta^t u(c_t,s_t)  \leq \frac{M}{1-\beta}. \]
Therefore, we should only look at possible value functions with $v(s)
\leq \frac{M}{1-\beta}$ for all $s$. To show that $T$ is a
contraction, we must define the space that $v$ lies in and its
norm. Given the boundedness of $v$, it is natural to look at
$\mathcal{L}^\infty(\R)$, the space of all bounded real-valued function
with norm $\norm{v} = \sup_{x \in \R} |v(x)|$. Consider
\begin{align*}
  T(v_0)(s)-T(v_1)(s) =  \left(u(c_0,s) + \beta v_0(s_0')  \right) -
  \left(u(c_1,s) + \beta v_1(s_1')  \right)
\end{align*} 
where $c_i,s_i'$ is the maximizer to 
\begin{align*}
  \max_{c_i,s_i'} u(c_i,s) + \beta v_i(s_i') \text{ s.t. }
  s_i'=g(c_i,s).
\end{align*}
We should assume something that ensures $c$ and $s'$ exist. Assuming
$c',s$ lie in a compact set would be sufficient. Notice that
\[ T v_0 (s) = u(c_0,s) + \beta v_0(s_0')  \geq u(c_1,s) + \beta
v_0(s_1'). \]
Therefore,
\[ T(v_0)(s) - T(v_1)(s) \geq \beta (v_0(s_1') - v_1(s_1'). \]
Similarly,
\[ T(v_0)(s)-T(v_1)(s) \leq \beta (v_0(s_0') - v_1(s_0'). \]
It follows that
\begin{align*}
  \norm{T(v_0)-T(v_1)} = & \sup_{s} \left\vert T(v_0)(s)-T(v_1)(s) \right\vert \\
  \leq & \sup_s \left\vert \beta(v_0(s) -v_1(s)) \right\vert = \beta
  \norm{v_0 - v_1}.
\end{align*}
Hence, $T$ is a contraction and has a unique fixed point $V$. 

\paragraph{Advantages of dynamic program} 
Dynamic programming and optimal control can both be used to solve the
same sort of problems. Optimal control has the advantage that it uses
very directly what we know about optimization in $\R^n$ and applies it
to infinite dimensional spaces. Dynamic programming has the advantage
that it lets us focus on one period at a time, which can often be
easier to think about than the whole sequence. Because it only
requires maximizing over a few variables at a time, dynamic
programming can be a much more efficient way to calculate
solutions. The computational advantage of dynamic programming is
especially pronounced when some of the variables being maximized over
are discrete. We will see some examples of this below. 

\subsection{Solving dynamic programs} 

There are three ways to solve a dynamic program. They are:
\begin{enumerate}
\item Guess and verify the form of the value function
\item Iterate the Bellman equation analytically
\item Iterate the Bellman equation numerically
\end{enumerate}
If you guess correctly, the first method is fairly
straightforward. However, guessing correctly is difficult and often is
not possible at all. The second method will always work, but may not
lead to a closed form expression, and can be tedious. The third method
is the main way dynamic programs are solved in practice, but we will
not go into the details.

\begin{example}[Optimal growth by guessing and verifying]
  Consider an economy with a single infinitely lived representative
  consumer with per-period log utility from consumption and a discount
  factor of $\delta$. The economy's production function is
  Cobb-Douglas with capital as the only input. Anything not consumed
  at time $t$ becomes capital at time $t+1$. The optimal growth
  problem is
  \begin{align*}
    \max_{\{c_t\}_{t=0}^\infty} & \sum_{t=0}^\infty \delta^t
    \log(c_t) \\
    \text{s.t. } & c_t + k_{t+1} = k_t^\alpha.
  \end{align*}
  If we use the constraint to solve for $c_t$ and substitute into the
  objective, then we have
  \begin{align*}
    \max_{\{k_t\}_{t=1}^\infty} & \sum_{t=0}^\infty \delta^t
    \log(k_t^\alpha - k_{t+1}) \\
    \text{s.t.} & 0 \leq k_{t+1} \leq k_t^\alpha 
  \end{align*}
 The Bellman equation for this problem is
 \begin{align*}
   v(k) = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') + \delta
   v(k')
 \end{align*}
 Now, we guess the functional form of $v$. Since the per-period
 utility function is logarithmic and production is Cobb-Douglas, it is
 sensible to guess that $v(k) = c_0 + c_1 \log(k^a)$ where $c_0$,
 $c_1$, and $a$ are each constant for which we solve. Now, since $c_1
 \log(k^a) = c_1 a \log(k)$, $a$ and $c_1$ are redundant, so we can
 get rid of $a$, and just guess that $v(k) = c_0 + c_1 \log(k)$. 
 
 We now use the Bellman equation to solve for $c_0$ and $c_1$. First
 we solve for the optimal $k'$ for a given $c_0$ and $c_1$. The
 Bellman equation is:
 \begin{align*}
   c_0 + c_1 \log k = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha-k') +
   \delta\left(c_0 + c_1 \log k' \right).
 \end{align*}   
 We could write the Lagrangean with the constraints that $k'\geq 0$
 and $k'\leq k^\alpha$. If we were not sure whether these constraints
 would bind we would include them in the Lagrangean and check the
 complementary slackness conditions. However, it is slightly easier to
 just notice that these constraints cannot bind because utility
 approaches $-\infty$ as $k$ approaches $k^\alpha$ and the next
 period's value approaches $-\infty$ as $k'$ approaches $0$, so
 neither constraint will bind.  Without the constraints, the first
 order condition is:
 \begin{align*}
   -\frac{1}{k^\alpha - k'}  + \delta c_1 \frac{1}{k'} = & 0 \\
   -k' + \delta c_1 (k^\alpha - k') & = 0 \\
   k' = & \frac{\delta c_1}{1+\delta c_1} k^\alpha
 \end{align*}
 Now, we plug this back into the Bellman equation and solve for $c_0$
 and $c_1$ by varying $k$. 
 \begin{align*}
   c_0 + c_1 \log k = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha-k') +
   \delta\left(c_0 + c_1 \log k' \right) \\
   = & \log\left(k^\alpha - \frac{\delta c_1}{1+\delta c_1}
     k^\alpha\right) + \delta \left(c_0 + c_1 \log \left(\frac{\delta
         c_1}{1+\delta c_1} k^\alpha\right) \right) \\
   = & \log\left(\frac{1}{1+\delta c_1}\right) + \alpha \log k +
   \delta\left(c_0 + c_1 \log\left(\frac{\delta
         c_1}{1+\delta c_1} \right) + \alpha \log k \right) \\
   = & \underbrace{\log\left(\frac{1}{1+\delta c_1}\right) + \delta
     c_1 \log\left(\frac{\delta c_1}{1+\delta c_1} \right)}_{=c_0} +
   \underbrace{(\alpha + \delta c_1 \alpha )}_{= c_1} \log k
 \end{align*}
 Both the left and right sides of this equation are affince function
 of $\log k$. They can only be equal for all $k$ if the coefficients
 are equal. Thus,
 \begin{align*} 
   c_1 = & \alpha + \delta c_1 \alpha \\
   c_1 = & \frac{\alpha}{1-\delta \alpha}
 \end{align*}
 and 
 \begin{align*}
   c_0 = & \log\left(\frac{1}{1+\delta c_1}\right) + \delta c_1
   \log\left(\frac{\delta c_1}{1+\delta c_1} \right) \\
   = & -\left(1 + \delta \frac{\alpha}{1-\delta \alpha}\right) 
   \log\left(1+\delta  \frac{\alpha}{1-\delta \alpha}\right) + 
   \delta \frac{\alpha}{1-\delta \alpha} 
   \log\left(\delta \frac{\alpha}{1-\delta \alpha} \right) \\
   = & \log(1-\delta \alpha) + \frac{\delta \alpha}{1-\delta \alpha}
   \log(\delta \alpha).
 \end{align*}
 Finally, we should make sure that this solution doesn't violate the
 constraint. We have
 \[ k' = \frac{\delta c_1}{1+\delta c_1} k^\alpha = \delta \alpha
 k^\alpha, \]
 so the constraints are satisfied as long  as $\delta \alpha \in
 (0,1)$.  
\end{example}

If we cannot guess the form of the value function, we can try to find
it by repeatedly applying the Bellman operator. The Bellman operator
is the $T$ operator we defined above,
\[ T(v)(s) = \max_{c,s'} u(c,s) + \beta v_0(s') \text{ s.t. } s'=g(c,s). \]
We already showed that $T$ is a contraction (provided $u$ is bounded
and $\abs{\beta} < 1$). Among other things, this means that if we
start with an arbitrary guess of the value function, $v_0$, and then
construct a sequence by repeatedly applying $T$, i.e., 
\[ v_i = T(v_{i-1}), \]
then the sequence $v_i$ will converge to a unique fixed point, $v$,
that satisfies the Bellman equation. 
\begin{example}[Optimal growth by iterating]
  The same optimal growth problem as in the previous example can also
  be solved by iterating the Bellman operator. We start with
  \emph{any} guess of the value function for $v_0$. A common choice is
  the zero function, $v_0(k) = 0$ for all $k$. Then we find $v_1$ by
  solving 
  \begin{align*}
    v_1(k) = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') + \delta v_0(k)
    \\
    = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') \\
    = & \alpha \log k.
  \end{align*}
  Then, we repeat to find $v_2$.
  \begin{align*}
    v_2(k) = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') + \delta v_1(k)
    \\
    = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') + \delta
    \alpha \log(k')  \\
    = & c_2 + (\alpha + \delta \alpha^2) \log k,
  \end{align*}
  where $c_2$ is some constant that involves $\delta$, $\alpha$, and
  their logs. The third equality comes from writing the first order
  condition, solving for $k'$, and subsituting back into the
  objective.  We can explicitly solve for $c_2$, but it doesn't matter
  for the first order condition for $v_3$, so we don't need to know it
  exactly. We repeat again to get $v_3$
  \begin{align*}
    v_3(k) = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') + \delta v_1(k)
    \\
    = & \max_{k' \in [0,k^\alpha]} \log(k^\alpha - k') + \delta
    \alpha \log(k')  \\
    = & c_3 + (\alpha + \delta \alpha^2 + \delta^2 \alpha^3) \log k.
  \end{align*}    
  We could repeat again to get 
  \begin{align*}
    v_4(k) = & c_4 + (\alpha + \delta \alpha^2 + \delta^2 \alpha^3 +
    \delta^3 \alpha^4) \log k \\
    v_5(k) = & c_5 + (\alpha + \delta \alpha^2 + \delta^2 \alpha^3 +
    \delta^3 \alpha^4 + \delta^4 \alpha^5) \log k \\
    \vdots
  \end{align*}
  etc. Eventually, we hopefully notice a pattern. The more obvious
  pattern is that each $v_i$ and will always be of the form $v_i(k) =
  c_i + m_i \log(k)$. Thus, we know that $v(k)$ will have that same
  form and we can go back to the guess and verify method. Better yet,
  we could notice that
  \[ v_i(k) = c_i + \alpha \sum_{j = 0}^i (\alpha \delta)^j \log(k),\]
  so 
  \begin{align*}
    v(k) = C + \frac{\alpha}{1-\alpha \delta} \log k.
  \end{align*}
  If we care about $C$, we could find it by either explicitly writing
  $c_i$ in terms of $\delta$ and $\alpha$ and taking the limit; or
  using the guess verify method just for $C$. 
\end{example}

Solving for the value function, whether by guessing and verifying or
iterating can be a bit tedious. Even worse, for most specifications of
the per-period payoff $u$ and constraints $g$, there will be no closed
form solution for $v$. That makes it impossible to guess the form, and
iterating the Bellman equation will not lead to a discernible pattern
(although it will still give a convergent series). Using a computer to
solve for the value function avoids both these problems. A computer
does not care that Bellman operator iteration is tedious, and it can
numerically compute $v(k)$ even if it has no closed form. 

Another situation where dynamic programs can be solved analytically is
when the control variable is discrete. For example, a person could be
choosing to work or not each period, or a firm could be choosing to
enter or exit a market. The section below and the last problem on
problem set 6 are examples of dynamic programming with discrete
control variables. 


\clearpage
\bibliographystyle{jpe}
\bibliography{../526}

\end{document}
