\documentclass[compress]{beamer} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{verbatim}
%\usepackage[small,compact]{titlesec} 
%\usecolortheme{beaver}
\usetheme{Hannover}
%\usecolortheme{whale}

%\usepackage{pxfonts}
%\usepackage{isomath}
%\usepackage{mathpazo}
%\usepackage{arev} %     (Arev/Vera Sans)
%\usepackage{eulervm} %_   (Euler Math)
%\usepackage{fixmath} %  (Computer Modern)
%\usepackage{hvmath} %_   (HV-Math/Helvetica)
%\usepackage{tmmath} %_   (TM-Math/Times)
\usepackage{tgheros}
%\usepackage{cmbright}
%\usepackage{ccfonts} \usepackage[T1]{fontenc}
%\usepackage[garamond]{mathdesign}
\usepackage{color}
\usepackage{ulem}

\setbeamertemplate{navigation symbols}{}
\AtBeginSection[] % Do nothing for \section*
{ \frame{\sectionpage} }

\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\def\inprobLOW{\rightarrow_p}
\def\inprobHIGH{\,{\buildrel p \over \rightarrow}\,} 
\def\inprob{\,{\inprobHIGH}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
\def\F{\mathbb{F}}
\def\R{\mathbb{R}}
\newcommand{\gmatrix}[1]{\begin{pmatrix} {#1}_{11} & \cdots &
    {#1}_{1n} \\ \vdots & \ddots & \vdots \\ {#1}_{m1} & \cdots &
    {#1}_{mn} \end{pmatrix}}
\newcommand{\iprod}[2]{\left\langle {#1} , {#2} \right\rangle}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\renewcommand{\det}{\mathrm{det}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\spn}{\mathrm{span}}
\newcommand{\row}{\mathrm{Row}}
\newcommand{\col}{\mathrm{Col}}
\renewcommand{\dim}{\mathrm{dim}}
\newcommand{\prefeq}{\succeq}
\newcommand{\pref}{\succ}


\title{Vector spaces}
\author{Paul Schrimpf}
\institute{UBC \\ Economics 526}
\date{\today}

\begin{document}

\frame{\titlepage}
\setcounter{tocdepth}{2}

\begin{frame}
  \frametitle{Overview}
  \begin{itemize} 
  \item Main idea: take intuition from $\R^3$ and apply to $\R^n$ and
    other vector spaces
  \end{itemize}
\end{frame}

\begin{frame}
  \tableofcontents  
\end{frame}

\section{Normed vector spaces}
\begin{frame}
  \frametitle{Normed vector spaces}
  \begin{itemize}
  \item Measure length or distance
  \end{itemize}
  \begin{definition}
    A \textbf{normed vector space}, $(V,\F,+,\cdot, \norm{\cdot})$, is a
    vector space with a function, called the \textbf{norm}, from $V$ to
    $\F$ and denoted by $\norm{v}$ with the following properties:
    \begin{enumerate}
    \item $\norm{v} \geq 0$ and $\norm{v} = 0$ iff $v = 0$,
    \item $\norm{\alpha v} = |\alpha|\norm{v}$ for all $\alpha \in \F$,
    \item The triangle inequality holds:
      \[ \norm{v_1+v_2} \leq \norm{v_1} + \norm{v_2} \]
      for all $v_1, v_2 \in V$.
    \end{enumerate}
  \end{definition}
  \begin{itemize}
  \item Shortest distance between two points is a straight line
  \end{itemize}
\end{frame}

\subsection{Examples}
\begin{frame}
  \frametitle{Examples}
  \begin{example}
    $\R^3$ is a normed vector space with norm
    \[ \norm{x} = \sqrt{x_1^2 + x_2^2 + x_3^2}. \] 
    This norm is exactly
    how we usually measure distance. For this reason, it is called the
    Euclidean norm.
    
    More generally, for any $n$, $\R^n$, is a normed vector space with
    norm 
    \[ \norm{x} = \sqrt{\sum_{i=1}^n x_i^2 }. \]
  \end{example}
\end{frame}

\begin{frame}
  \frametitle{Examples}
  \begin{example}
    $\R^n$ with the norm 
    \[ \norm{x}_p = \left( \sum_{i=1}^p |x_i|^p \right)^{1/p} \]
    for $p \in [1,\infty]$\footnote{Where $\norm{x}_\infty = \max_{1\leq
        i \leq n} |x_i| $} is a normed vector space. This norm is called
    the p-norm. 
  \end{example}
\end{frame}  

\begin{frame}
  \frametitle{Examples}
  \begin{example}
    $\mathcal{L}^p(0,1)$ with p-norm 
    \[ \norm{f}_p = \left(\int_0^1 |f(x)|^p dx\right)^{1/p} \] is a
    normed vector space. Moreover, $\mathcal{L}^p(0,1)$ is a different
    space for different $p$. For example, $\frac{1}{x^{1/p}} \not\in
    \mathcal{L}^p(0,1)$, but $\frac{1}{x^{1/p}} \in \mathcal{L}^q(0,1)$
    for $q < p$. 
  \end{example}
\end{frame}

\section{Inner product spaces}
\begin{frame}
  \frametitle{Inner product spaces}
  \begin{itemize}
  \item Measure angles
  \end{itemize}
  \begin{definition}
    A real \textbf{inner product space} is a vector space over the field
    $\R$ with an additional operation called the inner product that is
    function from $V \times V$ to $\mathbb{R}$. We denote the inner
    product of $v_1, v_2 \in V$ by $\iprod{v_1}{v_2}$. It has the
    following properties:
    \begin{enumerate}
    \item Symmetry: $\iprod{v_1}{v_2} = \iprod{v_2}{v_1}$
    \item Linear: $\iprod{a v_1 + b v_2}{v_3} = a \iprod{v_1}{v_3} + b
      \iprod{v_2}{v_3}$ for $a, b \in \R$
    \item Positive definite: $\iprod{v}{v} \geq 0$ and equals $0$ iff
      $v=0$. 
    \end{enumerate} 
  \end{definition}
  \begin{itemize}
  \item Inner product space is also a normed vector space
    \[ \norm{x} = \sqrt{\iprod{x}{x}}. \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example}
  \begin{itemize}
  \item $\R^n$ with the dot product,
    \[ \iprod{x}{y} = x\cdot y = \sum_{i=1}^n x_i y_i = x^T y \] 
  \item Norm induced by the inner product is the Euclidean norm
    \[ \norm{x} = \sqrt{\iprod{x}{x}} = \sqrt{\sum_{i=1}^n x_i^2 } \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Measuring angles}
  \begin{itemize}
  \item \begin{align*}
      \norm{x + y}^2 = & \iprod{x+y}{x+y} \\
      = & \iprod{x}{x} + 2\iprod{x}{y} + \iprod{y}{y}
    \end{align*}
  \item In $\R^n$ with the Euclidean norm when $x$ and $y$ are at
    right angles 
    to one other, $\iprod{x}{y} = 0$, and we have the Pythagorean theorem:
    \[ \norm{x + y}^2 = \norm{x}^2 + \norm{y}^2. \]
  \end{itemize}
  
  \begin{definition}
    Let $x,y \in V$, an inner product space. $x$ and $y$ are
    \textbf{orthogonal} iff $\iprod{x}{y} = 0$. 
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Measuring angles}
  \begin{theorem}\label{thm:angle}
    Let $u, v \in \R^n$, then the angle between them is
    \[ \theta = \cos^{-1} \frac{\iprod{u}{v}}{\norm{u}\norm{v}}. \]  
  \end{theorem}
  \begin{proof}
    Draw picture.
    \[ \cos \theta = \frac{\norm{tv}}{\norm{u}}. \]
    Use Pythagorean theorem,
    \begin{align*}
      \norm{u}^2 = & \norm{tv}^2 + \norm{u - tv}^2 \\
      \norm{u}^2 = & t^2 \norm{v}^2 + \norm{u}^2 - 2t\iprod{u}{v} + t^2
      \norm{v}^2 
      \\
      2 t \iprod{u}{v} = & 2 t^2 \norm{v}^2 
    \end{align*}
  \end{proof}
\end{frame}

\subsection{Useful inequalities}

\begin{frame}
  \frametitle{Useful inequalities}
  \begin{itemize}
  \item Triangle inequality
  \end{itemize}
  \begin{theorem}[Reverse triangle inequality]
    Let $V$ be a normed vector space and $x,y \in V$. Then
    \[ \left| \norm{x} - \norm{y} \right| \leq \norm{x-y}. \]
  \end{theorem}
  \begin{proof}
    By the usual triangle inequality,
    \begin{align*}
      \norm{x} + \norm{x-y} \geq & \norm{y} \\
      \norm{x-y} \geq \norm{y} - \norm{x}
    \end{align*}
    and
    \begin{align*}
      \norm{y} + \norm{y-x} \geq & \norm{x} \\
      \norm{y-x} \geq \norm{x} - \norm{y}.
    \end{align*}
  \end{proof}
\end{frame}
\begin{frame}
  \frametitle{Useful inequalities}
  
  \begin{theorem}[Cauchy-Schwarz inequality \label{thm:cauchy-schwarz}]
    Let $V$ be an inner product space and let $u,v\in V$. Then,
    \[ \left\vert \iprod{u}{v} \right\vert \leq \norm{u}\norm{v}. \]
  \end{theorem}
  \begin{proof}
    Setup as before, we can show that $t =
    \frac{\iprod{u}{v}}{\norm{v}^2}$. Now, let $z = u-tv$. By the
    Pythagorean theorem,
    \begin{align*}
      \norm{u}^2 = & \norm{tv}^2 + \norm{z}^2 \\
      = & \frac{\iprod{u}{v}^2}{\norm{v}^2} + \norm{z}^2 
    \end{align*}
    $\norm{z}^2\geq 0$, so $
    \norm{u}^2 \geq \frac{\iprod{u}{v}^2}{\norm{v}^2} 
    \norm{u}\norm{v} \geq |\iprod{u}{v}|.$
  \end{proof}
\end{frame}

\section{Projections}

\begin{frame}
  \frametitle{Projections}
  \begin{definition}
    Let $V$ be an inner product space and $x,y \in V$. The
    \textbf{projection} of $y$ onto $x$ is 
    \[ P_x y = \frac{\iprod{y}{x}}{\norm{x}^2} x. \]
    More generally, the projection of $y$ onto a finite set $\{x_1, x_2,
    ... , x_k\}$ is 
    \[ P_{\{x_j\}_j=1^k} y = \sum_{j=1}^k P_{x_j - \sum_{i=1}^{j-1}
      P_{x_i} x_j} y. \]
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Projections}
  \begin{definition}
    More generally still, if $X \subseteq V$ is a linear subspace, then
    the projection of $y$ onto $X$ is
    \[ P_{X} y = P_{\{b_j\}_{j=1}^k} y \]    
    where $b_j \in X$ and $b_1, ..., b_k$ span $X$. 
    
    Finally, if $Y \subseteq V$ the projection of $Y$ onto $X$ is just
    the set consisting of the projection of each element of $y$ onto
    $X$, i.e.
    \[ P_{X} Y = \{ P_{X} y : y \in Y \}. \]
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Projections}
  \begin{lemma}
    Any projection is an idempotent linear transformation. 
  \end{lemma}
  \begin{proof}
    \begin{itemize}
    \item Verify that projections have the two properties required
      for them to be linear transformations.
    \item Show that projections are idempotent. 
      \begin{align*}
        P_x (P_x y) = & P_x\left(\frac{\iprod{x}{y}}{\norm{x}^2} x \right)
        \\
        = & \frac{\iprod{x}{\frac{\iprod{x}{y}}{\norm{x}^2}
            x}}{\norm{x}^2} x \\ 
        = & \frac{\iprod{x}{y}}{\norm{x}^2}
        \frac{\iprod{x}{x}}{\norm{x}^2} x \\
        = & P_x y.
      \end{align*}
    \end{itemize}
  \end{proof}
\end{frame}

\section{Row, column, and null space}

\subsection{Row space}
\begin{frame}[shrink]
  \frametitle{Row space}
  \begin{definition}
    Let $A$ be an $m$ by $n$ matrix. The \textbf{row space} of $A$,
    denoted $\row (A)$, is the space spanned by the row vectors of $A$.
  \end{definition}
  \begin{itemize}
  \item $\row(A) \subseteq \R^n$
  \end{itemize}
  \begin{lemma}
    Performing Gaussian elimination does not change the row space of a
    matrix. 
  \end{lemma}
  \begin{proof}
    Let $a_{1},...,a_m$ be the row vectors of $A$. Each step of Gaussian
    elimination transforms some $a_j$ into $a_j + g a_k$ with $k \neq j$
    or $g \neq -1$. Can show that
    \[ \spn(a_1,...,a_m) = \spn(a_1 + g a_k , ..., a_m). \]
  \end{proof}
  \begin{corollary}\label{cor:rankdimrow}
    The dimension of the row space of a matrix is equal to its rank.
  \end{corollary}
\end{frame}

\subsection{Column space}

\begin{frame}
  \frametitle{Column space}
  \begin{definition}
    Let $A$ be an $m$ by $n$ matrix. The \textbf{column space} of $A$,
    denoted $\col(A)$, is the space spanned by the column vectors of
    $A$.
  \end{definition}
  \begin{itemize}
  \item $\col{A} \subseteq \R^m$
  \end{itemize}
  \begin{lemma}
    Let $A$ be an $m$ by $n$ matrix. Then $A x = b$ has a solution iff $b
    \in \col(A)$.
  \end{lemma}
\end{frame}

\begin{frame}
  \begin{definition}
    A column of a matrix, $A$, is \textbf{basic} if the corresponding
    column of the row echelon form, $A_r$, contains a pivot.
  \end{definition}
  \begin{theorem}
    The basic columns of $A$ form a basis for $\col(A)$. 
  \end{theorem}
  \begin{proof}
    Let $A$ be $m \times n$ and denote its columns as $v_1,...,v_n$. Let
    $A_r$ be the row echelon form of $A$ and denotes its columns as
    $w_1,...,w_n$. Let $w_{i_1}, ..., w_{i_k}$ be the basic columns of
    $A_r$. Each has more zeros, so $w_{i_1},...,w_{i_k}$
    are linearly independent. By definition of row echelon form, the
    final $m-k$ rows of $A_r$ are all zero. Therefore $\dim \col(A_r) \leq
    k$, and $w_{i_1},...,w_{i_k}$ must be a basis for $\col(A_r)$.
  \end{proof}    
\end{frame}
\begin{frame}
  \begin{proof}[Continued]
    Now we show that $v_{i_1}, ..., v_{i_k}$ are a basis for
    $\col(A)$. Suppose
    \[ c_1 v_{i_1} + ... + c_k v_{i_k} = 0. \]
    Then we could do Gaussian elimination to convert this system to
    \[ c_1 w_{i_1} + ... + c_k w_{i_k} = 0. \]
    $w_{i_1},... , w_{i_k}$ are linearly independent so $c_1 = 0
    ,... c_k = 0$.
    
    Add any other $v_j$, $j \not\in \{i_1, ..., i_k\}$,
    then by the same argument there must exist a non-zero $c$ than
    solves 
    \[ c_1 v_{i_1} + ... + c_k v_{i_k} + c_j v_j = 0. \]
    Thus, $v_{i_1} , ..., v_{i_k}$ is a basis for $\col(A)$. 
  \end{proof}
\end{frame}

\begin{frame}
  \begin{corollary}
    The dimensions of the row and column spaces of any matrix are
    equal. 
  \end{corollary}
  \begin{corollary}
    $\rank A = \rank A^T$.
  \end{corollary}
\end{frame}

\subsection{Null space}
\begin{frame}
  \frametitle{Null space}
  \begin{definition}
    Let $A$ be $m$ by $n$. The set of solutions to the homogeneous
    equation $Ax = 0$ is the \textbf{null space} (or kernel) of
    $A$, denoted by $\mathcal{N}(A)$ (or $\mathrm{Null}(A)$).
  \end{definition}
  
  \begin{definition}
    Let $V \subseteq \R^n$ be a linear subspace, and let $c \in \R^n$ be
    a fixed vector. The set
    \[ \{ x \in \R^n: x = v + c \text{ for some } v \in V \} \]
    is called the set of \textbf{translates} of $V$ by $c$, and is
    denoted $c + V$. Any set of
    translates of a linear subspace is called an \textbf{affine} space. 
  \end{definition}
\end{frame}

\begin{frame}
  \begin{lemma}
    Let $A x = b$ be an $m$ by $n$ system of linear equations. Let $x_0$
    be any particular solution. Then the set of solutions is $x_0 +
    \mathcal{N}(A)$. 
  \end{lemma}
  \begin{proof}
    Let $w \in x_0 + \mathcal{N}(A)$. Then 
    \begin{align*}
      Aw = & A(x_0) + A(\underbrace{w-x_0}_{\in \mathcal{N}(A)}) \\
      = & b + 0.
    \end{align*}
    Let $w$ be a solution to $Ax = b$. Then
    \begin{align*}
      A(w - x_0) = Aw - Ax_0 = 0
    \end{align*}
    so $w - x_0 \in \mathcal{N}(A)$ and $w \in x_0 + \mathcal{N}(A)$. 
  \end{proof}
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $A$ be an $m$ by $n$ matrix. Then 
    $\dim \mathcal{N}(A) = n - \rank A$
  \end{theorem}
  \begin{proof}
    \begin{itemize}
    \item Let $u_1, ..., u_k$ be a basis for $\mathcal{N}(A)$. We can add
      $u_{k+1},...,u_n$ to $u_1, ..., u_k$ to form a basis for
      $\R^n$.
    \item Show that  $A u_{k+1}, ..., A u_n$ are a basis for the
      column space
      \begin{itemize}
      \item linearly independent 
      \item span $\col A$.
      \end{itemize}
    \end{itemize}
  \end{proof}
\end{frame}

\begin{frame}
  \begin{theorem}[Rouch\'{e}-Capelli] \label{thm:rc} A system of linear
    equations with $n$ variables has a solution if and only if the rank
    of its coefficient matrix, $A$, is equal to the rank of its
    augmented matrix, $\hat{A}$. Equivalently, a solution exists if and
    only if $b \in \col(A)$.
    
    If a solution exists and $\rank A$ is equal to its number of
    columns, the solution is unique. If a solution exists and $\rank A$
    is less than its number of columns, there are infinite solutions. In
    this case the set of solutions forms is $x_0 + \mathcal{N}(A)$,
    where $x_0$ is any particular solution to $A x = b$. This set of
    solutions is an affine subspace of dimension $n - \rank A$.
  \end{theorem}
\end{frame}

\begin{frame}\frametitle{Relationship among row, column, and null
    spaces}
  \begin{itemize}
  \item $\col(A) = \row(A^T) \subseteq \R^m$
  \item $\row(A) = \col(A^T) \subseteq \R^n$
  \item $\mathcal{N}(A) \subseteq \R^n$ and $\mathcal{N}(A^T)
    \subseteq \R^m$
  \item Let $x \in \mathcal{N}(A)$, $y \in \row(A)$, what is
    $\iprod{x}{y}$? 
  \end{itemize}
\end{frame}

\begin{frame}[shrink]
  \frametitle{Relationship among row, column, and null
    spaces}
  \begin{itemize}
  \item If $x \in \mathcal{N}(A)$, $y \in \row(A) = \col(A^T)$, then
    $\iprod{x}{y} = 0$. 
  \item $\mathcal{N}(A)$ and $\row(A)$ are orthogonal subspaces of $\R^n$
  \item If $x \in \mathcal{N}(A^T)$, $y \in \row(A^T) = \col(A)$, then
    $\iprod{x}{y} = 0$. 
  \item $\mathcal{N}(A^T)$ and $\col(A)$ are orthogonal subspaces of
    $\R^m$
  \item $\forall x \in \R^n$, 
    \begin{align*}
      A x = & A (P_{\row{A}}x + P_{\mathcal{N}(A)} x)  \\
      = &  A (P_{\row{A}}x) + A (P_{\mathcal{N}(A)} x)  \\
      = & A (P_{\row{A}})x \in \col(A) = \row(A^T)
    \end{align*}
  \item $\forall w \in \R^m$,
    \begin{align*}
      A^T w = & A^T (P_{\col{A}}w + P_{\mathcal{N}(A^T)} w)  \\
      = &  A^T (P_{\col{A}}w) + A^T (P_{\mathcal{N}(A^T)} w) \\
      = & A^T (P_{\col{A}})w \in \row(A) = \col(A^T)
    \end{align*}    
  \end{itemize}
\end{frame}

\end{document}
