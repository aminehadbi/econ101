\documentclass[compress]{beamer} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{verbatim}
%\usepackage[small,compact]{titlesec} 
%\usecolortheme{beaver}
\usetheme{Hannover}
%\usecolortheme{whale}

%\usepackage{pxfonts}
%\usepackage{isomath}
%\usepackage{mathpazo}
%\usepackage{arev} %     (Arev/Vera Sans)
%\usepackage{eulervm} %_   (Euler Math)
%\usepackage{fixmath} %  (Computer Modern)
%\usepackage{hvmath} %_   (HV-Math/Helvetica)
%\usepackage{tmmath} %_   (TM-Math/Times)
\usepackage{tgheros}
%\usepackage{cmbright}
%\usepackage{ccfonts} \usepackage[T1]{fontenc}
%\usepackage[garamond]{mathdesign}
\usepackage{color}
\usepackage{ulem}

\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\def\inprobLOW{\rightarrow_p}
\def\inprobHIGH{\,{\buildrel p \over \rightarrow}\,} 
\def\inprob{\,{\inprobHIGH}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
\def\F{\mathbb{F}}
\def\R{\mathbb{R}}
\newcommand{\gmatrix}[1]{\begin{pmatrix} {#1}_{11} & \cdots &
    {#1}_{1n} \\ \vdots & \ddots & \vdots \\ {#1}_{m1} & \cdots &
    {#1}_{mn} \end{pmatrix}}
\newcommand{\iprod}[2]{\left\langle {#1} , {#2} \right\rangle}
\renewcommand{\det}{\mathrm{det}}
\newcommand{\rank}{\mathrm{rank}}

% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{conjecture}{Conjecture}[section]
% \newtheorem{corollary}{Corollary}[section]
% \newtheorem{lemma}{Lemma}[section]
% \newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{question}{Question}
% \newtheorem{assumption}{}[section]
% %\renewcommand{\theassumption}{C\arabic{assumption}}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{step}{Step}[section]
% \newtheorem{remark}{Comment}[section]
% \newtheorem{example}{Example}[section]
% \newtheorem*{example*}{Example}

%\linespread{1.1}

%\renewcommand{\sectionmark}[1]{\markright{#1}{}}


\title{Matrix algebra and introduction to vector spaces}
\author{Paul Schrimpf}
\institute{UBC \\ Economics 526}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}
  \frametitle{Correction from last time}  
  \begin{theorem}[Rouch\'{e}-Capelli] \label{thm:rc} A system of
    linear equations with $n$ variables has a solution if and only
    if the rank of its coefficient matrix, $A$, is equal to the rank
    of its augmented matrix, $\hat{A}$. If a solution exists and
    $\rank A$ is equal to its number of columns, the solution is
    unique. If a solution exists and $\rank A$ is less than its
    number of columns 
    \[ 
    \{s+x^* \in \mathbb{R}^n : s \in S \text{ and } Ax^* = b \} 
    \]
    where $S$ is a linear subspace of dimension $n - \rank A$ given by
    the set of solutions to $A x =0$, and $x^*$ is a solution to $A x
    = b$.
  \end{theorem}
\end{frame}

\begin{frame}
  \frametitle{Overview}
  
  \tableofcontents
\end{frame}

\section{Vector spaces and linear transformations}

\subsection{Vector spaces}

\begin{frame}

\begin{definition}
  A \textbf{vector space} is a set $V$ and a field $\mathbb{F}$ with
  two operations, addition $+$, which takes two elements of $V$ and
  produces another element in $V$, and scalar multiplication $\cdot$,
  which takes an element in $V$ and an element in $\mathbb{F}$ and
  produces an element in $V$, such that
  \begin{enumerate}
  \item $(V, +)$ is a commutative group, i.e. addition is close,
    associative, invertible, and commutative.
  \item Scalar multiplication has the following properties:
    \begin{enumerate}
    \item Closure: $\forall v \in V$ and $f \in \F$ we have $vf \in V$
    \item Distributivity: $\forall v_1 , v_2 \in V$ and $f_1, f_2 \in
      \F$
      \begin{align*}
        f_1 (v_1 + v_2) = f_1 v_1 + f_1 v_2 
      \end{align*}
      and 
      \begin{align*}
        (f_1 + f_2)v_1 = f_1 v_1 + f_2 v_1
      \end{align*}
    \item Consistent with field multiplication: $\forall v \in V$ and
      $f_1, f_2 \in V$ we have
      \begin{align*}
        1 v = v
      \end{align*}
      and 
      \begin{align*}
        (f_1 f_2) v =f_1 (f_2 v)
      \end{align*}
    \end{enumerate}
  \end{enumerate}  
\end{definition}
\end{frame}

\subsubsection{Examples}
\begin{frame}
  \begin{example}[Euclidean space]\label{ex:Rn}
    $\R^n$ over the field $\R$ is a vector space.  Vector addition and
    multiplication are defined in the usual way. If $\mathbf{x}_1 =
    (x_{11}, ..., x_{n1})$ and $\mathbf{x}_2 = (x_{12}, ..., x_{n2})$,
    then
    \[ \mathbf{x}_1 + \mathbf{x}_2 = (x_{11}+x_{12}, ... , x_{n1} +
    x_{n2}). \]
    Scalar multiplication is defined as
    \[ a \mathbf{x} = (a x_1, ..., ax_n) \] for $a \in \R$ and
    $\mathbf{x} \in R^n$. 
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    Any linear subspace of $\R^n$.
  \end{example}
  
  \begin{example}
    $(\mathbb{Q}^n, \mathbb{Q}, +, \cdot)$ is a vector space
    where $+$ and $\cdot$ defined as in \ref{ex:Rn}.
  \end{example}
  
  \begin{example}
    $(\mathbb{C}^n, \mathbb{C}, +, \cdot)$ where $+$ and $\cdot$ defined
    as in \ref{ex:Rn} except with complex addition and multiplication
    taking the place of real addition and multiplication. 
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    Take $V = \R^+$. Define ``addition'' as $x \oplus y = xy$ and define
    ``scalar multiplication'' as $\alpha \odot x = x^\alpha$. Then
    $(\R^+,\R, \oplus, \odot)$ is a vector space with identity element
    $1$.   
  \end{example}
\end{frame}

\begin{frame}\frametitle{Vector spaces of functions}
  \begin{example} \label{ex:funcSpace}
    Let $V = $ all functions from $[0,1]$ to $\R$. For $f, g \in V$,
    define $f + g$ by $(f+g)(x) = f(x) + g(x)$. Define scalar
    multiplication as $(\alpha f)(x) = \alpha f(x)$. Then this is a
    vector space. 
  \end{example}
  
  \begin{example}
    The set of all continuous functions with addition and scalar
    multiplication defined as in \ref{ex:funcSpace}.
  \end{example}
  
  \begin{example}
    The set of all $k$ times continuously differentiable functions with
    addition and scalar multiplication defined as in \ref{ex:funcSpace}.
  \end{example}
\end{frame}  

\begin{frame}
  \begin{example}
    The set of all polynomials with addition and scalar
    multiplication defined as in \ref{ex:funcSpace}.
  \end{example}
  
  \begin{example} 
    The set of all polynomials of degree at most $d$ with addition and scalar
    multiplication defined as in \ref{ex:funcSpace}.
  \end{example}  

  \begin{example}
    The set of all functions from $\R \to \R$ such that $f(29481763) =
    0$ with addition and scalar multiplication defined as in
    \ref{ex:funcSpace}.
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}\label{ex:LP}
    Let $1 \leq p < \infty$ and let $\mathcal{L}^p(0,1)$ be the set of
    functions from $(0,1)$ to $\R$ such that $\int_0^1 |f(x)|^p dx$ is
    finite.  Then $\mathcal{L}^p (0,1)$ with the field $\R$ and
    addition and scalar multiplication defined as
    \begin{align*}
      (f + g)(x) = & f(x) + g(x) \\
      (\alpha f)(x) = & \alpha f(x)
    \end{align*} 
    is a vector space. 
  \end{example}
\end{frame}

\subsubsection{Linear combinations}

\begin{frame}
  \begin{definition}
    Let $V$ be a vector space and $v_1,..., v_k \in V$. A linear
    combination of $v_1,..., v_k$ is any vector 
    \[c_1 v_1 + ... + c_k v_k \]
    where $c_1, ..., c_k \in \F$. 
  \end{definition}
  \begin{question} 
    How can we be sure that $c_1 v_1 + ... + c_k v_k \in V$?
  \end{question}
\end{frame}

\begin{frame}
  \begin{definition}
    Let $V$ be a vector space and $W \subseteq V$. The
    \textbf{span} of $W$ is the set
    of all finite linear combinations of elements of $W$.
  \end{definition}

  \begin{lemma}
    The \textbf{span} of any  $W \subseteq V$ is a linear subspace.
  \end{lemma}
\end{frame}


\begin{frame}
  \begin{example}
    Let $V$ be the vector space of all functions from $[0,1]$ to $\R$ as
    in example \ref{ex:funcSpace}. The span of $\{1, x, ..., x^n\}$ is
    the set of all polynomials of degree less than or equal $n$.
  \end{example}
\end{frame}

\begin{frame}
  \begin{definition}
    A set of vectors $v_1, ..., v_k \in V$, is \textbf{linearly
      independent} if the only solution to
    \begin{align*}
      \sum_{j=1}^k c_j v_j = 0 
    \end{align*}
    is $c_1 = c_2 = ... = c_k = 0$. 
  \end{definition}
\end{frame}

\subsubsection{Dimension and basis}

\begin{frame}
  \begin{definition}
    The \textbf{dimension} of a vector space, $V$, is the cardinality of
    the largest set of linearly independent elements in $V$.
  \end{definition} 
  
  \begin{definition}
    A \textbf{basis} of a vector space $V$ is any set of linearly
    independent vectors $b_1, ..., b_k$ such that the span of $b_1, ...,
    b_k$ is $V$.
  \end{definition}
\end{frame}

\begin{frame}
  \begin{example}
    A basis for $\R^n$ is $e_1 = (1, 0, ..., 0 )$, $e_2 = (0, 1, 0, ...,
    0)$, $...$, $e_n = (0, ... , 0 , 1)$. This basis is called the
    standard basis of $\R^n$. 
  \end{example}

  \begin{example} 
    What is the dimension of each of the examples of vector spaces
    above? Can you find a basis for them? 
  \end{example}
\end{frame}

\begin{frame}\frametitle{Basis gives coordinates}
  \begin{lemma}
    Let $\{b_1, ..., b_k\}$ be a basis for a vector space $V$. Then
    $\forall v \in V$ there exists a unique $v_1, ..., v_k \in \F$ and
    such that $ v = \sum_{i=1}^k v_i b_i$  
  \end{lemma}
  \begin{proof}
    \begin{itemize}
    \item $B$ spans $V$, so such $(v_1, ..., v_k)$ exist. 
    \item Suppose there exists another such $(v_1', ..., v_k')$. Then
      \begin{align*}
        v = \sum v_i b_i = & \sum v_i' b_i \\
        \sum v_i b_i - \sum v_i'b_i = & 0 \\
        \sum (v_i - v_i)' b_i = & 0.
      \end{align*}
    \end{itemize}
  \end{proof}
\end{frame}

\begin{frame}\frametitle{Dimension $ = |$Basis $|$}
   \begin{lemma}
    If $B$ is a basis for a vector space $V$ and $I \subseteq V$ is a
    set of linearly independent elements then $|I| \leq |B|$. 
  \end{lemma}

  \begin{corollary}
    Any two bases for a vector space have the same cardinality.
  \end{corollary}
\end{frame}

\begin{frame}
  \begin{definition}
    Let $V$ and $W$ be vector spaces over the field $\F$. $V$ and $W$ are
    \textbf{isomorphic} if there exists a one-to-one and onto function,
    $I:V \to W$ such that 
    \[ I(v^1 + v^2) = I(v^1) + I(v^2) \]
    for all $v^1, v^2 \in V$,
    and 
    \[ I(\alpha v) = \alpha I(v) \]
    for all $v \in V$, $\alpha \in \F$.
    Such an $I$ is called an \textbf{isomorphism}.
  \end{definition}
\end{frame}

\begin{frame}\frametitle{$\R^n$ is the ``unique'' $n$-dimensional
    vector space}
  \begin{theorem}
    Let $V$ be an $n$-dimensional vector space over the field $\F$. Then
    $V$ is isomorphic to $\F^n$. 
  \end{theorem}
\end{frame}

\section{Linear transformations}

\begin{frame}
  \begin{definition}
    A \textbf{linear transformation} (aka linear function) is a
    function, $A$, from a vector space $(V,\F,+,\cdot)$ to a vector
    space $(W,\F,+,\cdot)$ such that $\forall v_1, v_2 \in V$,
    \begin{align*}
      A (v_1 + v_2) = A v_1 + A v_2 
    \end{align*}
    and 
    \begin{align*}
      A (f v_1) = f A v_1
    \end{align*}
    for all scalars $f$.   
    \begin{itemize}
    \item Linear transformation from $V \to V$ is called a \textbf{linear
        operator}
    \item Linear transformation from $V \to \R$ is called a
      \textbf{linear functional}
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Examples}
  \begin{example}
    Any isomorphism
  \end{example}
  
  \begin{example}
    The identity operator: $I:V \to V$ defined by $I(v) = v$
  \end{example}

  \begin{example}
    The zero transformation: $0_T:V \to W$ defined by $0_T(v) = 0_w$
  \end{example}

  \begin{example}
    $f:\R^2 \to \R$ defined by $f((x_1,x_2) = x_1$
  \end{example}
\end{frame}


\begin{frame}
  \begin{theorem}
    For any linear transformation, $A$, from $\R^n$ to $\R^m$ there is an
    associated $m$ by $n$ matrix,
    \[ 
    \gmatrix{a}
    \]
    where $a_{ij}$ is defined by $A e_j = \sum_{i=1}^m a_{ij}
    e_i$. Conversely, for any $m$ by $n$ matrix, there is an associated
    linear transformation from $\R^n$ to $\R^m$ defined by $A e_j =
    \sum_{i=1}^n a_{ij}$.
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{proof}
    \begin{itemize} 
    \item Let $A$ be a linear transformation from $\R^n$ to
      $\R^m$
    \item $b_1, b_2, .., b_n$ basis for $\R^n$
    \item $\forall v \in V$ $\exists \alpha_j\in \R$ s.t. $v = \sum_{j=1}^n
      \alpha_j b_j$ 
    \item $A v = \sum_{j=1}^n \alpha_j A b_j$ so only need $A b_j$ to
      determine $A$
    \item $d_1, ..., d_m $ basis for $\R^m$ , so 
      \[
      A b_j = \sum_{i=1}^m a_{ij} d_i.
      \]
    \end{itemize}
  \end{proof}
\end{frame}

\begin{frame}
  \frametitle{Other examples of linear transformations}
  \begin{example}[Integral operator]
    Let $k(x,y)$ be a function from $(0,1)$ to $(0,1)$ such that
    $\int_0^1 \int_0^1 k(x,y)^2 dx dy$ is finite.  Define
    $K:\mathcal{L}^2(0,1) \rightarrow \mathcal{L}^2(0,1)$ by
    \begin{align*}
      (K f) (x) = \int_0^1 k(x,y) f(y) dy
    \end{align*}
    Then $K$ is a linear transformation. 
  \end{example}
\end{frame}

\begin{frame}[shrink]
  \frametitle{Other examples of linear transformations}
  \begin{example}[Conditional expectation]
    $X$ and $Y$ are real valued random variables with joint pdf $f_{xy}(x,y)$
    and marginal pdfs $f_x(x) = \int_\R f(x,y) dy$ and $f_y(y) =
    \int_\R f(x,y) dx$. Define the vector spaces 
    \[ 
    V = \mathcal{L}^2(\R,f_y) = \{g:
    \R \to \R \text{ such that } \int_{\R} f_y(y) g(y)^2 dy <
    \infty \} \]
    and 
    \[ 
    W = \mathcal{L}^2(\R,f_x) = \{g:
    \R \to \R \text{ such that } \int_{\R} f_x(x) g(x)^2 dx <
    \infty \} 
    \]  
    The conditional expectation function is $\mathcal{E}: V \to W$
    defined by 
    \begin{align*}
      (\mathcal{E} g)(x) = E[g(Y) | X = x] = & \int_\R
      \frac{f_{xy}(x,y)}{f_x(x) f_y(y)} g(y) f_y(y) dy.
    \end{align*}
  \end{example}
\end{frame}

\begin{frame}
  \frametitle{Other examples of linear transformations}
  \begin{example}[Differential operator]
    Let $C^\infty(0,1)$ be the set of all infinitely differentiable
    functions from $(0,1)$ to $\R$. It can easily be shown that
    $C^\infty(0,1)$ is a vector space. Let $D:C^\infty(0,1) \rightarrow
    C^\infty(0,1)$ be defined by 
    \[ (D f) (x) = \frac{d f}{dx}(x) \]
    Then $D$ is a linear transformation.
  \end{example}
\end{frame}

\section{Matrix operations and properties}

\subsection{Addition} 
\begin{frame}
  \frametitle{Addition}
  \begin{itemize}
  \item $A = \gmatrix{a}$, $B = \gmatrix{b}$ 
  \item Linear transformation  implies $(A+B)x = Ax + Bx$
    \begin{align*}
      (A + B) e_i = & A e_i + B e_i \\
      = & \sum_{j=1}^n a_{ij} e_j + \sum_{j=1}^n b_{ij} e_j\\
      = & \sum_{j=1}^n (a_{ij} + b_{ij}) e_j,
    \end{align*}
  \item so $A + B = \gmatrix{a + b}$.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Addition properties}
  \begin{enumerate}
  \item Associative: $A+(B + C) = (A+B) + C$,
  \item Commutative: $A + B = B + A$ ,
  \item Identity: $A + \mathbf{0} = A$, where $\mathbf{0}$ is an $m$ by
    $n$ matrix of zeros, and
  \item Invertible $A + (-A) = \mathbf{0}$ where $-A = \gmatrix{-a}$.
  \end{enumerate}
\end{frame}

\subsubsection{Scalar multiplication}
\begin{frame}
  \frametitle{Scalar multiplication}
  \begin{itemize}
  \item Linear transformation requires $A \alpha x = \alpha A x$
  \item so,
    \begin{align*}
      \alpha A = \begin{pmatrix} \alpha a_{11} & \cdots &
        \alpha a_{1n} \\ \vdots & \ddots & \vdots \\ \alpha a_{m1} & \cdots &
        \alpha a_{mn} \end{pmatrix}
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}\frametitle{The space of matrices is a vector space}
  \begin{itemize}
  \item $L(\R^n,\R^m) \equiv $ all $m$ by $n$ matrices $\equiv$ all
    linear transformations from $\R^n$ to $\R^m$ with addition and
    multiplication as above is a vector space
    \begin{itemize}
    \item \emph{Question}: $L(\R^n,\R^m)$ is isomorphic to what other
      vector space that we have seen?
    \end{itemize}
  \item $L(V,W) \equiv$ all linear transformations from $V \to W$ is a
    vector space
  \end{itemize}
\end{frame}

\subsection{Matrix multiplication}

\begin{frame}[shrink]
  \frametitle{Matrix multiplication}
  \begin{itemize}
  \item Multiplication $ \equiv$ composition of linear transformations
  \item $A: \R^n \rightarrow \R^m$, $B:\R^p \rightarrow \R^n$.
  \item Consider $A(B e_k)$
    \begin{align*}
      A(B e_k) = & A (\sum_{j=1}^n b_{jk} e_i) \\
      = & \sum_{j=1}^n b_{jk} A e_i \\
      = & \sum_{j=1}^n b_{jk} \left(\sum_{l=1}^m a_{ij} e_l\right)  \\
      = & \sum_{l=1}^m \left(\sum_{j=1}^n a_{ij} b_{jk} \right) e_l \\
      = & \begin{pmatrix} 
        \sum_{j=1}^n a_{1j} b_{j1} & \cdots & \sum_{j=1}^n a_{1j} b_{jp} \\
        \vdots & \ddots & \vdots \\
        \sum_{j=1}^n a_{mj} b_{j1} & \cdots & \sum_{j=1}^n a_{mj} b_{jp}
      \end{pmatrix} e_l \\
      = & (AB) e_l.
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Multiplication properties}
  \begin{enumerate}
  \item Associative: $A(BC) = (AB) C$
  \item Distributive: $A(B+C) = AB + AC$ and $(A+B)C = AC + BC$
  \item Identity: $AI_n = A$ where $A$ is $m$ by $n$ and $I_n$ is the
    identity linear transformation from $\R^n$ to $\R^n$ such that $I_nx = x
    \forall x \in \R^n$
  \item Not commutative
  \end{enumerate}
\end{frame}

\subsection{Transpose}

\subsubsection{Transpose and inner products}
\begin{frame}
  \begin{definition}
    A real \textbf{inner product space} is a vector space over the field
    $\R$ with an additional operation called the inner product that is
    function from $V \times V$ to $\mathbb{R}$. We denote the inner
    product of $v_1, v_2 \in V$ by $\iprod{v_1}{v_2}$. It has the
    following properties:
    \begin{enumerate}
    \item Symmetry: $\iprod{v_1}{v_2} = \iprod{v_2}{v_1}$
    \item Linear: $\iprod{a v_1 + b v_2}{v_3} = a \iprod{v_1}{v_3} + b
      \iprod{v_2}{v_3}$ for $a, b \in \R$
    \item Positive definite: $\iprod{v}{v} \geq 0$ and equals $0$ iff
      $v=0$. 
    \end{enumerate}  
  \end{definition}
\end{frame}

\begin{frame}
  \begin{example}
    $\R^n$ with the \textbf{dot product}, $x cdot y = \sum_{i=1}^n x_i
    y_i$, is an inner product space. 
  \end{example}
  
  \begin{example}
    $\mathcal{L}^2(0,1)$ with $\iprod{f}{g} \equiv \int_0^1 f(x) g(x)
    dx$ is an inner product space.
  \end{example}
\end{frame}

\begin{frame}
  \frametitle{Transpose}
  \begin{definition}
    Given a linear transformation, $A$, from a real inner product space
    $V$ to a real inner product space $W$, the
    \textbf{transpose} of $A$, denoted $A^T$ (or often $A'$) is a
    linear transformation from $W$ to $V$ such that $\forall v \in V, w
    \in W$
    \begin{align*}
      \iprod{A v}{w} = \iprod{v}{A^T w}.
    \end{align*}
  \end{definition}
\end{frame}

\begin{frame}
  \frametitle{Transpose for matrices}
  \begin{itemize}
  \item 
    \begin{align*}
      \iprod{Av}{w} = & \sum_{i=1}^m \left(\sum_{j=1}^n a_{ij} v_j \right)
      w_i  \\
      = & \sum_{i=1}^m \sum_{j=1}^n a_{ij} w_i v_j
    \end{align*}
  \item  
    \begin{align*}
      \iprod{v}{A^T w} = & \sum_{j=1}^n v_j \left(\sum_{i=1}^m a_{ji}^T
        w_i\right) \\
      = & \sum_{i=1}^m \sum_{j=1}^n a_{ji}^T w_i v_j
    \end{align*}
  \item If $\iprod{A v}{w} = \iprod{v}{A^T w} $, for any $v$ and $w$ we must
    have $a_{ji}^T = a_{ij}$
  \item The transpose of a matrix simply swaps rows
    for columns
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Transpose properties}
  \begin{enumerate}
  \item $(A+B)^T = A^T + B^T$
  \item $(A^T)^T = A$
  \item $(\alpha A)^T = \alpha A^T$
  \item $(AB)^T = B^T A^T$.
  \item $\rank A = \rank A^T$
  \end{enumerate}
\end{frame}

\subsubsection{Transpose and dual spaces}

\begin{frame} \frametitle{Transpose and dual space}
  \begin{definition}
    Let $V$ be a vector space. The \textbf{dual space} of $V$, denote
    $V^\ast$ is the set of all (continuous) linear functionals,
    $v^\ast: V \to \R$.
  \end{definition}
  
  \begin{example}
    The dual space of $\R^n$ is the set of $1 \times n$ matrices. In
    fact, for any finite dimensional vector space, the dual space is the
    set of row vectors from that space.        
  \end{example}
  
\end{frame}

\begin{frame}
  \begin{example}
    Let $1 \leq p \leq \infty$, define 
    \[ \ell^p = \{ (x_1, x_2, ... ) : \sum_{i=1}^\infty |x_i|^p < \infty
    \} \]
    and 
    \[ \ell^\infty = \{(x_1, x_2, ... ) : \max_{i \in \mathbb{N}} |x_i| < \infty
    \} \]
    What is the dual space of $\ell_\infty$?  
  \end{example}
\end{frame}

\begin{frame}
\begin{example}
  Dual space of $V = \mathcal{L}^2(\R,f_x) = \{g:  \R \to \R \text{ such that } \int_{\R} f_x(x) g(x)^2 dx <
  \infty\}$? 
  \begin{itemize}
  \item Let $h \in \mathcal{L}^2(\R,f_x)$, define 
    \[ h^\ast(g) = \int_\R f_x(x) g(x) h(x) dx. \] 
    then if $h^\ast(g)$ is finite for all $g$, $h^\ast \in V^\ast$
  \item Can show $h^\ast$ is finite for $g$, $h$ \& $V^\ast = \{
    h^\ast : h \in V\}$
  \item The mapping $h \to h^\ast$ is an isomorphism between $V$ and
    $V^\ast$ 
  \end{itemize}
\end{example}
\end{frame}

\begin{frame}\frametitle{Dual space definition of transpose}
  \begin{definition}
    If $A: V \to W$ is a linear transformation, then the
    \textbf{transpose} (or dual) of $A$ is $A^T: W^\ast \to V^\ast$
    defined by $(A^Tw^\ast)v = w^\ast(Av)$.
  \end{definition}
  \begin{itemize}
  \item This definition is the same as the previous one when $V$ and
    $W$ are inner product spaces
    \begin{itemize}
    \item Show that if $V, W$ are inner product spaces then $V^\ast$ is
      isomorphic to $V$, $W^\ast$ is isomorphic to $W$
    \item Show definitions are same
    \end{itemize}
  \end{itemize}
\end{frame}


\subsection{Types of matrices}

\begin{frame}
  \frametitle{Types of matrices}
  \begin{definition}
    A \textbf{column} matrix is any $m$ by $1$ matrix.
  \end{definition}
  
  \begin{definition}
    A \textbf{row} matrix is any $1$ by $n$ matrix.
  \end{definition}
  
  \begin{definition}
    A \textbf{square} matrix has the same number of rows and columns.
  \end{definition}
\end{frame}

\begin{frame}
  \begin{definition}
    A \textbf{diagonal} matrix is a square matrix with non-zero entries
    only along its diagonal, i.e.\ $a_{ij} = 0$ for all $i \neq j$. 
  \end{definition}

  \begin{definition}
    An \textbf{upper triangular} matrix is a square matrix that has
    non-zero entries only on or above its diagonal, i.e.\ $a_{ij} = 0$
    for all $j>i$. A \textbf{lower triangular} matrix is the transpose
    of an upper triangular matrix.
  \end{definition}
  
  \begin{definition}
    A matrix is \textbf{symmetric} if $A = A^T$.
  \end{definition}
\end{frame}

\begin{frame}
  \begin{definition}
    A matrix is \textbf{idempotent} if $AA = A$.
  \end{definition}
  
  \begin{definition}
    A \textbf{permutation} matrix is a square matrix of $1$'s and $0$'s
    with exactly one $1$ in each row or column.  
  \end{definition}
  
  \begin{definition}
    A \textbf{nonsingular} matrix is a square matrix whose rank equals
    its number of columns.
  \end{definition}
  
  \begin{definition}
    An \textbf{orthogonal} matrix is a square matrix such that $A^TA =
    I$.
  \end{definition}
\end{frame}

\subsection{Invertibility}

\begin{frame}
  \frametitle{Invertibility}
  \begin{definition}
    Let $A$ be a linear transformation from $V$ to $W$. Let $B$ be a
    linear transfromation from $W$ to $V$. $B$ is a \textbf{right
      inverse} of $A$ if $AB = I_V$. Let $C$ be a linear tranfromation
    from $V$ to $W$. $C$ is a \textbf{left inverse} of $A$ if $CA = I_W$. 
  \end{definition}  
  
  \begin{lemma}
    If $A$ is a linear transformation from $V$ to $V$ and $B$ is a right
    inverse, and $C$ a left inverse, then $B = C$. 
  \end{lemma}
\end{frame}

\begin{frame}
  \begin{lemma}
    Let $A$ be a linear tranformation from $V$ to $V$, and suppose $A$
    is invertible. Then $A$ is nonsingular and the unique solution to
    $Ax = b$ is $x = A^{-1} b$. 
  \end{lemma}

  \begin{lemma}
    If $A$ is nonsingular, then $A^{-1}$ exists.
  \end{lemma}

  \begin{corollary}
    A square matrix $A$ is invertible if and only if $\rank A$ is
    equal to its number of columns.
  \end{corollary}
\end{frame}

\begin{frame}
  \frametitle{Properties of matrix inverse}
  \begin{enumerate}
  \item $(AB)^{-1} = B^{-1} A^{-1}$
  \item $(A^T)^{-1} = (A^{-1})^T$
  \item $(A^{-1})^{-1} = A$
  \end{enumerate}
\end{frame}

\section{Determinants}

\begin{frame}
  \frametitle{Determinants}
  \begin{itemize}
  \item Determinant: geometry and invertibility
  \item Invert 2 by 2 matrix by Gauss-Jordan elimination:
    \begin{align*}
      \begin{pmatrix} a & b & 1 & 0 \\
        c & d & 0 & 1 
      \end{pmatrix} \simeq & 
      \begin{pmatrix} a & b & 1 & 0 \\
        0 & \frac{ad-bc}{a} & -\frac{c}{a} & 1 
      \end{pmatrix} \\
      \simeq & 
      \begin{pmatrix} a & b & 1 & 0 \\
        0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}
      \end{pmatrix} \\
      \simeq & 
      \begin{pmatrix} a & 0 & \frac{ad}{ad-bc} & \frac{-ba}{ad-bc} \\
        0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}
      \end{pmatrix} \\
      \simeq & 
      \begin{pmatrix} 1 & 0 & \frac{d}{ad-bc} & \frac{-b}{ad-bc} \\
        0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}.
      \end{pmatrix}
    \end{align*}
  \item Needed $ad-bc \neq 0$.
  \end{itemize}
\end{frame}

\begin{frame}
  \includegraphics[width=0.5\linewidth]{det2}
\end{frame}

\begin{frame}
  \includegraphics[width=0.5\linewidth]{det3} 
\end{frame}

\begin{frame}
  \begin{definition}\label{deta}
    Let $A$ be an $n$ by $n$ matrix consisting of column vectors $a_1,
    ..., a_n$. The determinant of $A$ is the unique function such that
    \begin{enumerate}
    \item\label{d1} $\det I_n = 1$.
    \item\label{d2} As a function of the columnes, $\det$ is an
      alternating form: 
      $\det (A) = 0$ iff $a_1, ..., a_n$ are linearly dependent.
    \item\label{d3} As a function of the columnes, $\det$ is multi-linear:
      \[
      \det(a_1, ..., b a_j + c v, ..., a_n) = b\det(A) +
      c\det(a_1,...,v,...a_n) 
      \]
    \end{enumerate}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{itemize}
  \item \ref{d1} natural, needed for volume interpretation
  \item \ref{d2} ensures $\det A = 0$ iff $A$ singular
  \end{itemize}
  \begin{lemma}
    Let $A$ be an $n$ by $n$ matrix. The $A$ is singular if and only if
    the columns of $A$ are linearly dependent.
  \end{lemma}
  \begin{corollary}\label{cor:dns}
    $A$ is nonsingular if and only if $\det A \neq 0$.
  \end{corollary}
\end{frame}

\begin{frame}
  \begin{itemize}
  \item \ref{d3} is related to volume interpretation
  \item Consider diagonal matrices, volume interpretation require
    multi-linearity
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{definition}\label{detc}
    The \textbf{determinant} of a square matrix $A$ is defined
    recursively as
    \begin{enumerate}
    \item For $1$ by $1$ matrices, $\det A = a_{11}$
    \item For $n$ by $n$ matrices, 
      \[ \det A = \sum_{j=1}^n (-1)^{1+j} a_{1j} \det A_{-1,-j} \]
      where $A_{-i,-j}$ is the $n-1$ by $n-1$ matrix obtained by
      deleting the $i$th row and $j$th column of $A$.
    \end{enumerate}
  \end{definition}
  \begin{itemize}
  \item \textbf{minor:} $\det A_{-i,-j}$ 
  \item \textbf{cofactor:} $(-1)^{i+j} \det A_{-i,-j}$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Determinant properties}
  \begin{theorem}
    The two definitions of the determinant, (\ref{deta}) and
    (\ref{detc}), are equivalent.
  \end{theorem}
  \begin{enumerate}
  \item $\det A^T = \det A$
  \item $\det (AB) = (\det A) (\det B)$
  \item $\det A^{-1} = (\det A)^{-1}$
  \item Usually, $\det(A + B) \neq \det A + \det B$
  \item If $A$ is diagonal, $\det A = \prod_{i=1}^n a_{ii}$
  \item\label{dpt} If $A$ is upper or lower triangular $\det A = \prod_{i=1}^n
    a_{ii}$.  
  \end{enumerate}
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $A$ be nonsingular. Then,
    \begin{enumerate}
    \item $A^{-1} = \frac{1}{\det A} \begin{pmatrix} 
        \det A_{-1,-1} & \cdots & (-1)^{1+n} \det A_{-n,-1} \\
        \vdots & \ddots & \vdots \\
        (-1)^{1+n} \det A_{-1,-n} & \cdots & (-1)^{n+n} \det A_{-n,-n} 
      \end{pmatrix}$
    \item (\textbf{Cramer's rule}) The unique solution to $A x = b$ is 
      \[ x_i = \frac{\det B_i}{\det A} \]
      where $B_i$ is the matrix $A$ with the $i$th column replaced by
      $b$. 
    \end{enumerate}
  \end{theorem}
\end{frame}

\section{Computational efficiency}

\begin{frame}
  \frametitle{Computational efficiency}
  \begin{itemize}
  \item Calculate determinant as defined above in $d(n)$ steps
    \begin{align*}
      d(n) = & n d(n-1) + 2n  \\
      = & 2n! \sum_{k=1}^n \frac{1}{(n-k)!} 
    \end{align*}
  \item Big O notation: $d(n) = O(f(n))$ if $\exists n_0$ such
    that
    \[
    d(n) \leq M f(n) 
    \]
    for some constant $M$ and all $n \geq n_0$
  \item $d(n) = O(n!)$
  \item Cramer's formula $=O((n+1)!)$
  \end{itemize}
  
\end{frame}

\begin{frame}
  \begin{itemize}
  \item Gaussian elimination in $g(n)$ steps
    \begin{align*}
      g(n) = & 2 \sum_{k=1}^n k(k-1) \\
      = & \frac{2}{3} (n^3 - n)
      = & O(n^3)
    \end{align*}
  \item Back substitute: $\sum_{k=1}^n k = \frac{1}{2}
    n(n-1)$ step
  \item Total: $O(n^3)$
  \end{itemize}  
\end{frame}

\section{Matrix decompositions}
\begin{frame}
  \frametitle{Matrix decompositions}
  \begin{definition}
    The \textbf{LU decomposition} of a square matrix is a decomposition
    of the form
    \[ P A = L U \]
    where $P$ is a permutation matrix, $L$ is lower triangular, and $U$
    is upper triangular. 
  \end{definition}
\end{frame}

\begin{frame}  
  \begin{definition}
    The \textbf{Cholesky decomposition} of a square, symmetric, and
    positive definite ($\det A > 0$) matrix $A$ is
    \[ A = L L^T \] 
    where $L$ is lower triangular with positive entries on the
    diagonal. 
  \end{definition}
\end{frame}

\begin{frame}  
\begin{definition}
  The \textbf{QR decomposition} of an $m$ by $n$ matrix is 
  \[ A = QR \]
  where $R$ is $m$ by $n$ and upper triangular and $Q$ is $m$ by $m$
  and orthogonal (i.e.\ $Q^T = Q^{-1}$).
\end{definition}
\end{frame}

\begin{frame}  
\begin{definition}
  The \textbf{singular value decomposition} of a matrix $m$ by $n$
  matrix $A$ is
  \[ A = U \Sigma V \]
  where $U$ is $m$ by $m$ and orthogonal, $\Sigma$ is $m$ by $n$ and
  diagonal, and $V$ is $n$ by $n$ and orthogonal.
\end{definition}
\end{frame}

\end{document}
