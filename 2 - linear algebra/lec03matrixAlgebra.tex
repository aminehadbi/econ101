\documentclass[12pt,reqno]{amsart}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage[left=1in,right=1in,top=0.9in,bottom=0.9in]{geometry}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{fancyhdr}
%\usepackage[small,compact]{titlesec} 

%\usepackage{pxfonts}
%\usepackage{isomath}
\usepackage{mathpazo}
%\usepackage{arev} %     (Arev/Vera Sans)
%\usepackage{eulervm} %_   (Euler Math)
%\usepackage{fixmath} %  (Computer Modern)
%\usepackage{hvmath} %_   (HV-Math/Helvetica)
%\usepackage{tmmath} %_   (TM-Math/Times)
%\usepackage{cmbright}
%\usepackage{ccfonts} \usepackage[T1]{fontenc}
%\usepackage[garamond]{mathdesign}
\usepackage{color}
\usepackage{ulem}

\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\def\inprobLOW{\rightarrow_p}
\def\inprobHIGH{\,{\buildrel p \over \rightarrow}\,} 
\def\inprob{\,{\inprobHIGH}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
\def\F{\mathbb{F}}
\def\R{\mathbb{R}}
\newcommand{\gmatrix}[1]{\begin{pmatrix} {#1}_{11} & \cdots &
    {#1}_{1n} \\ \vdots & \ddots & \vdots \\ {#1}_{m1} & \cdots &
    {#1}_{mn} \end{pmatrix}}
\newcommand{\iprod}[2]{\left\langle {#1} , {#2} \right\rangle}
\renewcommand{\det}{\mathrm{det}}
\newcommand{\rank}{\mathrm{rank}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{assumption}{}[section]
%\renewcommand{\theassumption}{C\arabic{assumption}}
\newtheorem{definition}{Definition}[section]
\newtheorem{step}{Step}[section]
\newtheorem{remark}{Comment}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{example*}{Example}

\linespread{1.1}

\pagestyle{fancy}
%\renewcommand{\sectionmark}[1]{\markright{#1}{}}
\fancyhead{}
\fancyfoot{} 
%\fancyhead[LE,LO]{\tiny{\thepage}}
\fancyhead[CE,CO]{\tiny{\rightmark}}
\fancyfoot[C]{\small{\thepage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\small{\thepage}} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

\makeatletter
\renewcommand{\@maketitle}{
  \null 
  \begin{center}%
    \rule{\linewidth}{1pt} 
    {\Large \textbf{\textsc{\@title}}} \par
    {\normalsize \textsc{Paul Schrimpf}} \par
    {\normalsize \textsc{\@date}} \par
    {\small \textsc{University of British Columbia}} \par
    {\small \textsc{Economics 526}} \par
    \rule{\linewidth}{1pt} 
  \end{center}%
  \par \vskip 0.9em
}
\makeatother

\title{Matrix algebra and introduction to vector spaces}
\date{\today}

\begin{document}

\maketitle

We have already used matrices in our study of systems of linear
equations. Matrices are useful through economics. You will see a lot
of matrices in econometrics. Understanding matrices is also essential
for understanding systems of nonlinear equations, which appear in all
fields of economics. This lecture will go over some fundamental
properties of matrices. It is based on a combination of Simon and
Blume Chapters 8-9, Appendix A of \textit{Econometric Analysis} by
William Green, and Chapter 9 of \textit{Principles of Mathematical
  Analysis} by Rudin. Carter sections 1.4.1 and 1.4.2 covers much the
same material as section 1.2 of these notes. The start of chapter 3 of
Carter (up to 3.1.1) covers much the same material as section 1.2 and
the start of section 2 of these notes. 

\section{Vector spaces and linear transformations}

Matrices are often introduced as just arrays of numbers.  That is how
matrices are typically defined. That sort of definition works out
alright. However, it makes certain subsequent definitions somewhat
mysterious. You might wonder why matrix multiplication is defined the
way it is, or why transposing is important.  Well, there is another,
more abstract, but also more fundamental way of describing
matrices. This way of looking at matrices will reveal exactly why
matrix multiplication is the way it is, and illuminate many other
properties of matrices. 

\subsection{Vector spaces}

To fully understand matrices, we must first under vector
spaces. Loosely, a vector space is a set whose elements can be added
and scaled. Vector spaces appear quite often in economics because many
economic quantities can be added and scaled. For example, if firm $A$
produces quantities $y_1^A$ and $y_2^A$ of goods  $1$ and $2$, while
firm $B$ produces $(y_1^B,y_2^B)$, then total production is
$(y_1^A+y_1^B, y_2^A+y_2^B)$. If firm $A$ becomes 10\% more
productive, then it will produce $(1.1 y_1^A, 1.1 y_2^A)$. 

\begin{definition}
  A \textbf{vector space} is a set $V$ and a field $\mathbb{F}$ with
  two operations, addition $+$, which takes two elements of $V$ and
  produces another element in $V$, and scalar multiplication $\cdot$,
  which takes an element in $V$ and an element in $\mathbb{F}$ and
  produces an element in $V$, such that
  \begin{enumerate}
  \item $(V, +)$ is a commutative group, i.e.
    \begin{enumerate}
    \item Closure: $\forall v_1 \in V$ and $v_2 \in V$ we have $v_1
      + v_2 \in V$. 
    \item Associativity: $\forall v_1, v_2, v_3 \in V$ we have $v_1
      + (v_2 + v_3 ) = (v_1 + v_2) + v_3 $. 
    \item Identity exists: $\exists 0 \in V$ such that $\forall v \in
      V$, we have $v + 0 = v$
    \item Invertibility: $\forall v \in V$ $\exists -v \in V$ such
      that $v + (-v) = 0$
    \item Commutativity: $\forall v_1, v_2 \in V$ we have $v_1+v_2 =
      v_2 + v_1$
    \end{enumerate}
  \item Scalar multiplication has the following properties:
    \begin{enumerate}
    \item Closure: $\forall v \in V$ and $f \in \F$ we have $vf \in V$
    \item Distributivity: $\forall v_1 , v_2 \in V$ and $f_1, f_2 \in
      \F$
      \begin{align*}
        f_1 (v_1 + v_2) = f_1 v_1 + f_1 v_2 
      \end{align*}
      and 
      \begin{align*}
        (f_1 + f_2)v_1 = f_1 v_1 + f_2 v_1
      \end{align*}
    \item Consistent with field multiplication: $\forall v \in V$ and
      $f_1, f_2 \in V$ we have
      \begin{align*}
        1 v = v
      \end{align*}
      and 
      \begin{align*}
        (f_1 f_2) v =f_1 (f_2 v)
      \end{align*}
    \end{enumerate}
  \end{enumerate}
\end{definition}
We now give some examples of vector spaces. 
\begin{example} \label{ex:Rn}
  $\R^n$ with the field $\R$ is a vector space. You are likely already
  familiar with this space. Vector addition and multiplication are
  defined in the usual way. If $\mathbf{x}_1 = (x_{11}, ..., x_{n1})$
  and $\mathbf{x}_2 = (x_{12}, ..., x_{n2})$, then vector addition is
  defined as
  \[ \mathbf{x}_1 + \mathbf{x}_2 = (x_{11}+x_{12}, ... , x_{n1} +
  x_{n2}). \]
  The fact that $(\R^n,+)$ is a commutative group follows from the
  fact that $(\R,+)$ is a commutative group. Scalar multiplication is
  defined as
  \[ a \mathbf{x} = (a x_1, ..., ax_n) \] for $a \in \R$ and
  $\mathbf{x} \in R^n$. You should verify that the three properties in
  the definition of vector space hold.  The vector space $(\R^n, \R,
  +, \cdot)$ is so common that it is called \textbf{Euclidean
    space}\footnote{To be more accurate, Euclidean space refers to
    $\R^n$ as an inner product space, which is a special kind of
    vector space that will be defined below.} We will often just refer
  to this space as $\R^n$, and it will be clear from context that we
  mean the vector space $(\R^n, \R, + , \cdot)$. In fact, we will
  often just write $V$ instead of $(V,\F,+,\cdot)$ when referring to a
  vector space.
\end{example}
One way of looking at vector spaces is that they are a way of trying
to generalize the things that we know about two and three dimensional
space to other contexts. 
\begin{example}
  Any linear subspace of $\R^n$ along with the field
  $\R$ and the usual vector addition and scalar multiplication is a
  vector space. Linear subspaces are closed under $+$ and $\cdot$ by
  definition. Linear subspaces inherit all the other properties
  required by the definition of a vector space from $\R^n$.
\end{example}

\begin{example}
  $(\mathbb{Q}^n, \mathbb{Q}, +, \cdot)$ is a vector space
  where $+$ and $\cdot$ defined as in \ref{ex:Rn}.
\end{example}

\begin{example}
  $(\mathbb{C}^n, \mathbb{C}, +, \cdot)$ where $+$ and $\cdot$ defined
  as in \ref{ex:Rn} except with complex addition and multiplication
  taking the place of real addition and multiplication. 
\end{example}
Except for the preceding two examples, all the vector spaces in this
class will be real vector spaces with the field $\mathbb{F} =
\mathbb{R}$. 

The following example might be a bit unfamiliar. It is a set of
functions. In economic theory, we might want to work with a set of
functions because we want to prove something for all functions in the
set. That is, we prove something for all utility functions or for all
production functions. In non-parametric econometrics, we try to
estimate an unknown function instead of an unknown finite dimensional
parameter. For example, instead of linear regression $y = x\beta +
\epsilon$ where want to estimate the unknown vector $\beta$, we might
say $y = f(x) + \epsilon$ and try to estimate the unknown function
$f$. Unfortunately, in many cases the set of all functions is too
large to prove what we want, so we must add some weak restriction to
the set to make it smaller. In the following example, we require
$\int_0^1 |f(x)|^p dx < \infty$. Other common restrictions include
requiring the function to be continuous or $k$-times continuously
differentiable.
\begin{example}\label{ex:LP}
  Let $1 \leq p < \infty$ and let $\mathcal{L}^p(0,1)$ be the set of
  functions from $(0,1)$ to $\R$ such that $\int_0^1 |f(x)|^p dx$ is
  finite.
  \footnote{For now, you can just think of $\mathcal{L}^p(0,1)$ as
    consisting of all functions from $(0,1) \to \R$. The $p$ and finite
    integral part only become important when we think of $\mathcal{L}^p$
    as normed vector spaces, which we will do in the next set of notes.}
  Then $\mathcal{L}^p (0,1)$ with the field $\R$ and addition
  and scalar multiplication defined as
  \begin{align*}
    (f + g)(x) = & f(x) + g(x) \\
    (\alpha f)(x) = & \alpha f(x)
  \end{align*} 
  is a vector space.  The only difficult part of the definition of
  vector spaces to verify is closure under addition. To prove closure
  under addition, we can use Jensen's inequality. This is a
  surprisingly useful inequality that we may or may not prove
  later. Anyway the simplest form of Jensen's inequality says that if
  $h(x)$ is convex\footnote{A real valued function $h(x)$ is (weakly)
    convex if for all $x_1,x_2$ and $\lambda \in (0,1)$ $h(\lambda x_1
    + (1-\lambda) x_2) \leq \lambda h(x_1) + (1-\lambda) h(x_2)$.},
  then for any $a_1>0$ and $a_2>0$,
  \[ h\left(\frac{a_1 x_1 + a_2 x_2}{a_1 + a_2} \right) \leq \frac{a_1
    h(x_1) + a_2 h(x_2) }{a_1 + a_2}. \]
  If $p>1$, $h(x) = |x|^p$ is convex. From Jensen's inequality,
  \begin{align*}
    | \frac{f(x) + g(x)}{2} |^p \leq & \frac{|f(x)|^p + |g(x)|^p}{2}
    \\
    |f(x) + g(x) |^p \leq 2^{p-1} |f(x)|^p + |g(x)|^p. 
  \end{align*}
  Integrating,
  \begin{align*}
    \int |f(x) + g(x) |^p dx \leq 2^{p-1} \left(\int|f(x)|^p dx + \int
      |g(x)|^p dx \right)
  \end{align*}
  The right side is finite for any $f, g \in \mathcal{L}^p(0,1)$, so
  the left side is also finite and $f+g \in \mathcal{L}^p(0,1)$. 
  
  To a get a better feel for $\mathcal{L}^p(0,1)$, you should try to
  come with some examples of familiar functions are or are not
  included. You could consider polynomials, rational functions (ratios
  of polynomials), exponential, and logarithm.
\end{example}

\begin{definition}
  Let $V$ be a vector space and $v_1,..., v_k \in V$. A \textbf{linear
    combination} of $v_1,..., v_k$ is any vector 
  \[c_1 v_1 + ... + c_k v_k \]
  where $c_1, ..., c_k \in \F$. 
\end{definition}

\begin{definition}
  Let $V$ be a vector space and $v_1,..., v_k \in V$. The
  \textbf{span} of $\{ v_1, ... , v_k \}$ is the set
  of all linear combinations of $v_1, ... , v_k$.
\end{definition}
\begin{lemma}
  The \textbf{span} of any  $v_1,..., v_k \in V$ is a linear subspace.
\end{lemma}
\begin{proof}
  Left as an exercise.
\end{proof}

\begin{example}
  For any $k\geq 0$, $x^k \in \mathcal{L}^p(0,1)$. The span of $\{1,
  x, ..., x^n\}$ is the set of all polynomials of degree less than or
  equal $n$. 
\end{example}

You might remember the next two definitions from the previous lecture.
\begin{definition}
  A set of vectors $v_1, ..., v_k \in V$, is \textbf{linearly
    independent} if the only solution to
  \begin{align*}
    \sum_{j=1}^k c_j v_j = 0 
  \end{align*}
  is $c_1 = c_2 = ... = c_k = 0$. 
\end{definition}

\begin{definition}
  The \textbf{dimension} of a vector space, $V$, is the cardinality of
  the largest set of linearly independent elements in $V$.
\end{definition} 

\begin{definition}
  A \textbf{basis} of a vector space $V$ is any set of linearly
  independent vectors $b_1, ..., b_k$ such that the span of $b_1, ...,
  b_k$ is $V$.
\end{definition}
If $V$ has a basis with $k$ elements, then the dimension of $V$ must
be at least $k$. In fact, we will soon see that dimension of $V$ must
be exactly $k$.
\begin{example}
  A basis for $\R^n$ is $e_1 = (1, 0, ..., 0 )$, $e_2 = (0, 1, 0, ...,
  0)$, $...$, $e_n = (0, ... , 0 , 1)$. This basis is called the
  standard basis of $\R^n$. 
\end{example}

\begin{example} 
  (Difficult) What is the dimension of $\mathcal{L}^p(0,1)$? Can you find a basis? 
\end{example}

\begin{lemma}
  Let $b_1, ... , b_k$ be a basis for a vector space $V$. Then
  $\forall v \in V$ there exists a unique $(v_1, ..., v_k)_ \in \F^k$
  such that $ v = \sum v_i b_i$  
\end{lemma}
\begin{proof}
  By the definition of a basis, $b_1, ..., b_k$ spans $V$, so such
  $(v_1, ..., v_k)$ must exist. Now suppose there exists another such
  $(v_1', ..., v_k')$. Then
  \begin{align*}
    v = \sum v_i b_i = & \sum v_i' b_i \\
    \sum v_i b_i - \sum v_i'b_i = & 0 \\
    \sum (v_i - v_i)' b_i = & 0.
  \end{align*}
  However, if $b_1,..., b_k$ is a basis, they must be linearly
  independent so $v_i = v_i'$ for each $i$.
\end{proof}

\subsection{Linear transformations}

\begin{definition}
  A \textbf{linear transformation} (aka linear function) is a
  function, $A$, from a vector space $(V,\F,+,\cdot)$ to a vector
  space $(W,\F,+,\cdot)$ such that $\forall v_1, v_2 \in V$,
  \begin{align*}
    A (v_1 + v_2) = A v_1 + A v_2 
  \end{align*}
  and 
  \begin{align*}
    A (f v_1) = f A v_1
  \end{align*}
  for all scalars $f$.   

  A linear transformation from $V$ to $V$ is called a \textbf{linear
    operator} on $V$. A linear transformation from $V$ to $\R$ is
  called a \textbf{linear functional} on $V$.
\end{definition}
Let 
\[ A = \gmatrix{a} \]
be a matrix. As when we were working with systems of linear equations,
let 
\[ A\mathbf{x} = 
\begin{pmatrix} 
  \sum_{j=1}^n a_{1j} x_j \\
  \vdots \\
  \sum_{j=1}^n a_{mj} x_j 
\end{pmatrix}, \] for $\mathbf{x} = (x_1, ..., x_n) \in \R^n$. Then
$A$ is a linear transformation from $\R^n$ to $\R^m$. You may want to
verify that $A(f_1 \mathbf{x}_1 + f_2 \mathbf{x}_2 ) = f_1 A
\mathbf{x}_1 + f_2 \mathbf{x}_2$ for scalars $f_1, f_2 \in \R$ and
vectors $\mathbf{x}_1, \mathbf{x}_2 \in \R^n$.

Conversely let $A$ be a linear transformation from $V$ to $W$ (if it
is helpful, you can let $V=\R^n$ and $W=\R^m$), and let $b_1, b_2,
..., b_n$ be a basis for $V$. By the definition of a basis, any $v \in
V$ can be written $v = \sum_{j=1}^n \alpha_j b_j$ for some $\alpha_j
\in \F$. By the definition of a linear transformation, we have
\begin{align*}
  A v = \sum_{j=1}^n \alpha_j A b_j. 
\end{align*}
Thus, a linear transformation is completely determined by its action
on a basis. Also, if $d_1, ..., d_m$ is a basis for $W$ then for each
$A b_j$ we must be able to write
\begin{align*}
  A b_j = \sum_{i=1}^m a_{ij} d_i.
\end{align*}
Thus, associated with a linear transformation there is an array of
$a_{ij} \in \F$ determined by the linear transformation (and choice of
basis). In the previous paragraph, we saw that conversely, if we have
an array of $a_{ij} \in \F$ we can construct a linear
transformation. This leads us to the following result.
\begin{theorem}
  For any linear transformation, $A$, from $\R^n$ to $\R^m$ there is an
  associated $m$ by $n$ matrix,
  \[ 
  \gmatrix{a}
  \]
  where $a_{ij}$ is defined by $A e_j = \sum_{i=1}^m a_{ij}
  e_i$. Conversely, for any $m$ by $n$ matrix, there is an associated
  linear transformation from $\R^n$ to $\R^m$ defined by $A e_j =
  \sum_{i=1}^n a_{ij} e_i$.
\end{theorem}
Thus, we see that matrices and linear transformations from $\R^m$ to
$\R^n$ are the same thing. This fact will help us make sense of many
of the properties of matrices that we will go through in the next
section. Also, it will turn out that most of the properties of
matrices are really properties of linear transformations. There are
linear transformations that cannot be represented by matrices, yet
many of the results and definitions that are typically stated for
matrices will apply to these sorts of linear transformations as
well. 

Two examples of linear transformations that cannot be represented by
matrices are integral and differential operators,
\begin{example}[Integral operator]
  Let $k(x,y)$ be a function from $(0,1)$ to $(0,1)$ such that
  $\int_0^1 \int_0^1 k(x,y)^2 dx dy$ is finite.  Define
  $K:\mathcal{L}^2(0,1) \rightarrow \mathcal{L}^2(0,1)$ by
  \begin{align*}
    (K f) (x) = \int_0^1 k(x,y) f(y) dy
  \end{align*}
  Then $K$ is a linear transformation. 
\end{example}
\begin{example}[Differential operator]
  Let $C^\infty(0,1)$ be the set of all infinitely differentiable
  functions from $(0,1)$ to $\R$. It can be shown that
  $C^\infty(0,1)$ is a vector space. Let $D:C^\infty(0,1) \rightarrow
  C^\infty(0,1)$ be defined by 
  \[ (D f) (x) = \frac{d f}{dx}(x) \]
  Then $D$ is a linear transformation.
\end{example}
We will not be studying these sort of linear transformations on
infinite dimensional spaces in much detail, but it may be useful to be
aware of them. Integral operators are very important when studying
differential equations.  They are also useful in many areas of
econometrics and in dynamic programming. We will encounter some linear
transformations on infinite dimensional spaces when we study optimal
control. 

\section{Matrix operations and properties}

Let $A$ and $B$ be linear transformations from $\R^m$ to $\R^n$ and
let $\gmatrix{a}$ and $\gmatrix{b}$ be the associated matrices.  Since
the linear transformation $A$ and the matrix $\gmatrix{a}$ represent
the same object, we will use $A$ to denote both.  From the previous
section, we know that $A$ and $B$ are characterized by their action on
the standard basis vectors in $\R^n$. In particular, $A e_j =
\sum_{i=1}^m a_{ij} e_i$ and $B e_j = \sum_{i=1}^m b_{ij} e_i$. 

\subsection{Operations}

\subsubsection{Addition} 
To define matrix addition, it makes sense to
require $(A+B)x = Ax + Bx$. Then,
\begin{align*}
  (A + B) e_i = & A e_i + B e_i \\
  = & \sum_{j=1}^n a_{ij} e_j + \sum_{j=1}^n b_{ij} e_j\\
  = & \sum_{j=1}^n (a_{ij} + b_{ij}) e_j,
\end{align*}
so the only way sensible way to define matrix addition is 
\begin{align*}
  A + B = \begin{pmatrix} a_{11} + b_{11} & \cdots &
    a_{1n} + b_{1n}  \\ \vdots & \ddots & \vdots \\ a_{m1} + b_{m1} & \cdots &
    a_{mn}+b_{mn} \end{pmatrix}
\end{align*}
As an exercise, you might want to verify that matrix addition has the
following properties:
\begin{enumerate}
\item Associative: $A+(B + C) = (A+B) + C$,
\item Commutative: $A + B = B + A$ ,
\item Identity: $A + \mathbf{0} = A$, where $\mathbf{0}$ is an $m$ by
  $n$ matrix of zeros, and
\item Invertible $A + (-A) = \mathbf{0}$ where $-A = \gmatrix{-a}$.
\end{enumerate}

\subsubsection{Scalar multiplication}
The definition of linear transformations requires that $A \alpha x =
\alpha A x$ where $\alpha \in \mathbb{F}$ and $x \in V$. To be
consistent with this, for matrices we must define
\begin{align*}
  \alpha A = \begin{pmatrix} \alpha a_{11} & \cdots &
    \alpha a_{1n} \\ \vdots & \ddots & \vdots \\ \alpha a_{m1} & \cdots &
    \alpha a_{mn} \end{pmatrix}
\end{align*}

\begin{example}
  Let $L(V,W)$ be the set of all linear transformations from $V$ to
  $W$. Define addition and scalar multiplication as above. Then 
  $L(V,W)$ is a vector space.
\end{example}

\subsubsection{Matrix multiplication}
Matrix multiplication is really the coposition of two linear
transformations. Let $A$ be a linear transfrom from $\R^n$ to $\R^m$
and $B$ be a linear tranformation from $\R^p$ to $\R^n$. Now, we
defined matrices by looking at how a linear tranformation acts on a
basis vectors, so to define multiplication, we should look at $A(B
e_k)$
\begin{align*}
  A(B e_k) = & A (\sum_{j=1}^n b_{jk} e_i) & \text{definition of $Be_k$}\\
  = & \sum_{j=1}^n b_{jk} A e_i & \text{Definition of linear
    transformtion} \\
  = & \sum_{j=1}^n b_{jk} \left(\sum_{l=1}^m a_{ij} e_l\right)  &
  \text{definition of $Ae_i$} \\
  = & \sum_{l=1}^m \left(\sum_{j=1}^n a_{ij} b_{jk} \right) e_l \\
  = & \begin{pmatrix} 
    \sum_{j=1}^n a_{1j} b_{j1} & \cdots & \sum_{j=1}^n a_{1j} b_{jp} \\
    \vdots & \ddots & \vdots \\
    \sum_{j=1}^n a_{mj} b_{j1} & \cdots & \sum_{j=1}^n a_{mj} b_{jp}
  \end{pmatrix} e_l \\
  = & (AB) e_l.
\end{align*}
The indexing in the above equations is unpleasant and could be
confusing. The important thing to remember is that matrix
multiplication is the composition of linear transformations. It then
makes sense that if $A$ is $m$ by $n$ (a transformation from $\R^n$ to
$\R^m$)and $B$ is $k$ by $l$ (a transformation from $R^l$ to $\R^k$),
we can only multiply $A$ times $B$ if $k = m$. Matrix multiplication
has the following properties:
\begin{enumerate}
\item Associative: $A(BC) = (AB) C$
\item Distributive: $A(B+C) = AB + AC$ and $(A+B)C = AC + BC$. 
\item Identity: $AI_n = A$ where $A$ is $m$ by $n$ and $I_n$ is the linear
  transformation from $\R^n$ to $\R^n$ such that $I_nx = x \forall x \in
  \R^n$.
\end{enumerate}
As we saw on the review, matrix multiplication is not commutative. 

\subsubsection{Transpose}

As you likely know, the transpose of an $m \times n$ matrix $A$ is an
$n \times m$, $A^T$ whose rows are the columns of $A$. As a linear
transformation, if $A: V \to W$, then $A^T: W \to V$, but how can we
define $A^T$ when $A$ cannot be represented as a matrix (i.e. when $V$
or $W$ is infinite dimensional). There are two ways to define the
transpose of a linear function. They each require introducing a new
concept. We will first define the transpose in terms of dual
spaces. Dual spaces do occasionally appear in economics, but they will
not be essential for the rest of this course (I did not cover them at
all last year). Then we will define the transpose in terms of inner
products. Inner products are very useful in their own right. However,
it is not essential to understand either of these abstract definitions
of the transpose. You will only really need to understand the
transposing of matrices.

\subsubsection{Transpose and dual spaces}

One way of defining the transpose of a linear transformation is in
terms of dual spaces.
\begin{definition}
  Let $V$ be a vector. The \textbf{dual space} of $V$, denote $V^\ast$
  is the set of all (continuous)\footnote{We have not yet defined
    continuity, so do not worry about this requirement. All linear
    functionals on finite dimensional spaces are continuous. Some
    linear functionals on infinite dimensional spaces are not
    continuous. The definition of dual space does not always require
    continuity. Of the dual space is defined as the set of all linear
    functionals, and the topological dual space is the set of all
    continuous linear functionals. We will ignore this distinction.}
  linear functionals, $v^\ast: V \to \R$.
\end{definition}

\begin{example}
  The dual space of $\R^n$ is the set of $1 \times n$ matrices. In
  fact, for any finite dimensional vector space, the dual space is the
  set of row vectors from that space. 
\end{example}

\begin{example}
  The space $\ell_p$ for $1 \leq p \leq \infty$ is the set of
  sequences of real numbers $\mathbf{x}=(x_1, x_2, ...)$ such that
  $\sum_{i=1}^\infty |x_i|^p < \infty$. (When $p = \infty$, $\ell_\infty = \{ 
  (x_1, x_2, ...) : \max_{i \in \mathbb{N}} |x_i| < \infty \}$). Such
  spaces appear in economics in discrete time, infinite horizon
  optimization problems. 

  Let's consider the dual space of $\ell_\infty$. In macro models, we
  rule out everlasting bubbles and ponzi schemes by requiring
  consumption divided by productivity to be in $\ell_\infty$. Every
  sequence, $\mathbf{p} = (p_1, p_2, ...) \in \ell_1$ gives rise to a linear
  functional on $\ell_\infty$ defined by
  \begin{align*}
    \mathbf{p}^\ast \mathbf{x} = \sum_{i=1}^\infty p_i x_i \leq
    \left(\sum_{i=1}^\infty |p_i| \right) \left(\max_{i \in
        \mathbb{N}} |x_i| < \infty\right). 
  \end{align*}
  We can conclude that $\ell_1 \subseteq \ell_\infty^\ast$. 

  As a (difficult) exercise, you could try to show whether or not
  $\ell_1 = \ell_\infty^\ast$. Exercise 3.46 of Carter is very
  related. 
\end{example}

\begin{definition}
  If $A: V \to W$ is a linear transformation, then the
  \textbf{transpose} (or dual) of $A$ is $A^T: W^\ast \to V^\ast$
  defined by $(A^Tw^\ast)v = w^\ast(Av)$.
\end{definition}
To parse this definition, note that $A^T w^\ast$ is an element of
$V^\ast$, so it is a linear transformation from $V$ to $\R$. Thus,
$(A^T w^\ast) v \in \R$. Similarly, $Av \in W$, and $w^\ast: W \to
\R$, so $w^\ast (A v) \in \R$. 

As an exercise, you should try to show that this definition of the
transpose is the same as the familiar flipping rows and columns
definition for matrices.

\subsubsection{Transpose and inner products}

Another way to define the transpose in terms of linear
transformations, is to use another property of Euclidean
space. 
\begin{definition}
  A real \textbf{inner product space} is a vector space over the field
  $\R$ with an additional operation called the inner product that is
  function from $V \times V$ to $\mathbb{R}$. We denote the inner
  product of $v_1, v_2 \in V$ by $\iprod{v_1}{v_2}$. It has the
  following properties:
  \begin{enumerate}
  \item Symmetry: $\iprod{v_1}{v_2} = \iprod{v_2}{v_1}$
  \item (Bi)linear: $\iprod{a v_1 + b v_2}{v_3} = a \iprod{v_1}{v_3} + b
    \iprod{v_2}{v_3}$ for $a, b \in \R$
  \item Positive definite: $\iprod{v}{v} \geq 0$ and equals $0$ iff
    $v=0$. 
  \end{enumerate}  
\end{definition}
Although it is not obvious from this definition, an inner product
space is a vector space where we can measure the angle between two
vectors. We will come back to this point next lecture.
\begin{example}
  $\R^n$ with the \textbf{dot product}, $x \cdot y = \sum_{i=1}^n x_i
  y_i$, is an inner product space. 
\end{example}

\begin{example}
  As it is usually defined, $\mathcal{L}^2(0,1)$ with $\iprod{f}{g}
  \equiv \int_0^1 f(x) g(x) dx$ is an inner product space. However, we
  have not defined $\mathcal{L}^2(0,1)$ carefully enough for this to
  be an inner product space. For example, consider $h(x)
  = \begin{cases} 0 \text{ if } x \neq 1/2 \\
    1 \text{ if } x = 1/2
  \end{cases}$. Then 
  \[ 
  \iprod{h}{h} = \int_0^1 h(x)^2 dx = 0,
  \]
  but $h(x)$ is not zero, violating the positive definite requirement
  of inner products. We can get around this problem by defining
  $\mathcal{L}^2(0,1)$ as not all functions, but as equivalence
  classes of functions. We say $f \simeq g$ if $\int_0^1 |f(x) -
  g(x)|^2 dx = 0$, and then define $\mathcal{L}^2(0,1)$ as the set of
  equivalence classes of functions. With this definition, the
  problematic $h$ above and $0$ represent the same element of
  $\mathcal{L}^2(0,1)$, and it becomes an inner product space. 

  As an exercise, you could try to verify some of these
  assertions. For example, show that $\simeq$ is really an equivalence
  relation, or show that $\mathcal{L}^2(0,1)$ remains a vector space
  when it is the set of equivalence classes instead of set of
  functions. Fully verifying these claims requires knowledge of
  measure theory, which is outside the scope of this course, but you
  could make some progress.
\end{example}

We can now define the transpose in terms of the inner product.
\begin{definition}
  Given a linear transformation, $A$, from a real inner product space
  $V$ to a real inner product space $W$, the
  \textbf{transpose} of $A$, denoted $A^T$ (or often $A'$) is a
  linear transformation from $W$ to $V$ such that $\forall v \in V, w
  \in W$
  \begin{align*}
    \iprod{A v}{w} = \iprod{v}{A^T w}.
  \end{align*}
\end{definition}
Now, let's take $V=\R^n$ and $W = \R^m$ and see what this definition
means for the transpose of a matrix. First observe that $A^T$ is a
mapping from $\R^m$ to $\R^n$, so as a matrix is must be $n$ by
$m$. Let $a^T_{ji}$ denote the entries of $A^T$. Rewriting the
inner product in terms of the entries of the matrix, we get
\begin{align*}
  \iprod{Av}{w} = & \sum_{i=1}^m \left(\sum_{j=1}^n a_{ij} v_j \right)
  w_i  \\
  = & \sum_{i=1}^m \sum_{j=1}^n a_{ij} w_i v_j
\end{align*}
and 
\begin{align*}
  \iprod{v}{A^T w} = & \sum_{j=1}^n v_j \left(\sum_{i=1}^m a_{ji}^T
    w_i\right) \\
  = & \sum_{i=1}^m \sum_{j=1}^n a_{ji}^T w_i v_j
\end{align*}
If $\iprod{A v}{w} = \iprod{v}{A^T w} $, for any $v$ and $w$ we must
have $a_{ji}^T = a_{ij}$. The transpose of a matrix simply swaps rows
for columns. The transpose has the following properties:
\begin{enumerate}
\item $(A+B)^T = A^T + B^T$
\item $(A^T)^T = A$
\item $(\alpha A)^T = \alpha A^T$
\item $(AB)^T = B^T A^T$.
\item $\rank A = \rank A^T$
\end{enumerate}

\subsection{Types of matrices}

There are some special types of matrices that will be useful.
\begin{definition}
  A \textbf{column} matrix is any $m$ by $1$ matrix.
\end{definition}

\begin{definition}
  A \textbf{row} matrix is any $1$ by $n$ matrix.
\end{definition}

\begin{definition}
  A \textbf{square} matrix has the same number of rows and columns.
\end{definition}

\begin{definition}
  A \textbf{diagonal} matrix is a square matrix with non-zero entries
  only along its diagonal, i.e.\ $a_{ij} = 0$ for all $i \neq j$. 
\end{definition}

\begin{definition}
  An \textbf{upper triangular} matrix is a square matrix that has
  non-zero entries only on or above its diagonal, i.e.\ $a_{ij} = 0$
  for all $j>i$. A \textbf{lower triangular} matrix is the transpose
  of an upper triangular matrix.
\end{definition}

\begin{definition}
  A matrix is \textbf{symmetric} if $A = A^T$.
\end{definition}

\begin{definition}
  A matrix is \textbf{idempotent} if $AA = A$.
\end{definition}

\begin{definition}
  A \textbf{permutation} matrix is a square matrix of $1$'s and $0$'s
  with exactly one $1$ in each row or column.  
\end{definition}

\begin{definition}
  A \textbf{nonsingular} matrix is a square matrix whose rank equals
  its number of columns.
\end{definition}

\begin{definition}
  An \textbf{orthogonal} matrix is a square matrix such that $A^TA =
  I$.
\end{definition}

\subsection{Invertibility}

\begin{definition}
  Let $A$ be a linear transformation from $V$ to $W$. Let $B$ be a
  linear transfromation from $W$ to $V$. $B$ is a \textbf{right
    inverse} of $A$ if $AB = I_V$. Let $C$ be a linear tranfromation
  from $V$ to $W$. $C$ is a \textbf{left inverse} of $A$ if $CA = I_W$. 
\end{definition}  

\begin{lemma}
  If $A$ is a linear transformation from $V$ to $V$ and $B$ is a right
  inverse, and $C$ a left inverse, then $B = C$. 
\end{lemma}
\begin{proof}
  Left multiply $AB = I$ by $C$ to get
  \begin{align*}
    CAB & = CI \\
    (CA) B & = C \\
    B & = C.
  \end{align*}
\end{proof}
\begin{corollary}
  If $A$ is a linear transformation from $V$ to $V$ and $B$ and $C$
  are either left or right inverses of $A$, then $B=C$.
\end{corollary}

\begin{definition} 
  Let $A$ be a linear transformation from $V$ to $V$. The
  \textbf{inverse} of $A$ is written $A^{-1}$ and satisfies $A A^{-1}
  = I = A^{-1} A$.
\end{definition}
The previous corollary shows that if $A^{-1}$ exists, it is unique. 

\begin{lemma}
  Let $A$ be a linear tranformation from $V$ to $V$, and suppose $A$
  is invertible. Then $A$ is nonsingular and the unique solution to
  $Ax = b$ is $x = A^{-1} b$. 
\end{lemma}
\begin{proof}
  Multiply both sides of the equation by $A^{-1}$ to get
  \begin{align*}
    A^{-1} A x =  & A^{-1} b \\
    x = & A^{-1} b
  \end{align*}
  Therefore a solution exists. This solution is also unique because
  any other solution, $x'$ must also satisfy 
  \begin{align*}
    A x' =  &  b \\
    A^{-1} A x'= & A^{-1} b \\
    x' = & A^{-1} b,
  \end{align*}
  and $A^{-1}$ is unique. 
\end{proof}

\begin{lemma}
  If $A$ is nonsingular, then $A^{-1}$ exists.
\end{lemma}
\begin{proof}
  Let $e_1, e_2, ... $ be a basis for $V$.  By the definition of
  nonsingular for any $e_j$ there exists a solution to 
  \[ A x = e_j. \]
  Call this solution $x_j$.  Define a linear transformation $C$ by 
  \[ C v = \sum v_j x_j, \]
  where $v = \sum v_j e_j $. We can write any $v \in V$ in this way
  since $e_j$ is a basis. Now, consider $A(Cv)$.
  \begin{align*}
    A (C v) = & A \sum v_j x_j \\
    = & \sum v_j A x_j \\
    = & \sum v_j e_j = v.
  \end{align*}
  Thus, $AC = I$ and $C = A^{-1}$.
\end{proof}
\begin{corollary}
  A square matrix $A$ is invertible if and only if $\rank A$ is equal
  to its number of columns.
\end{corollary}
\begin{proof}
  In the previous lecture, we showed that $\rank A = $ number of
  columns if and only if $A$ is nonsingular. That result along with
  the two previous lemmas implies the corollary.
\end{proof}

Let $A$ and $B$ be invertible square matrices. The inverse has the
following properties:
\begin{enumerate}
\item $(AB)^{-1} = B^{-1} A^{-1}$
\item $(A^T)^{-1} = (A^{-1})^T$
\item $(A^{-1})^{-1} = A$
\end{enumerate}

\section{Determinants}

Given a $1$ by $1$ matrix, say, $A = (a)$, we known that $A^{-1}$
exists if and only if $a \neq 0$. Also, viewed as a linear
transformation from $\R$ to $\R$, $A$ transforms $1$ into $a$. It
would be useful to have a number with similar properties for larger
matrices. That is, for any matrix $A$, we want a number that is not
zero if and only if $A^{-1}$ exists, and that describes something
about how $A$ acts as a linear transformation. Well, the determinant
of a matrix, written $\det A$ is exactly such a number. 

Let us start by looking at a 2 by 2 matrix, $A = \begin{pmatrix} a & b
  \\ c & d \end{pmatrix}$. The inverse of $A^{-1}$ must satisfy 
\begin{align}
  A A^{-1} = & I
\end{align}
If $x_1$ is the first column of $A^{-1}$ and $x_2$ is the second
column, then we can rewrite this as two systems of linear equations, 
\[ A x_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \]
and 
\[ A x_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}. \]
If you think about how we would solve these two systems using
Gauss-Jordan elimination, you will see that the steps involved depend
only on $A$ and not on the right side of the equation. Therefore, we
can solve both systems together by performing Gauss-Jordan elmination
on
\begin{align*}
  \begin{pmatrix} a & b & 1 & 0 \\
    c & d & 0 & 1 
  \end{pmatrix} \simeq & 
  \begin{pmatrix} a & b & 1 & 0 \\
    0 & \frac{ad-bc}{a} & -\frac{c}{a} & 1 
  \end{pmatrix} \\
  \simeq & 
  \begin{pmatrix} a & b & 1 & 0 \\
    0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}
  \end{pmatrix} \\
  \simeq & 
  \begin{pmatrix} a & 0 & \frac{ad}{ad-bc} & \frac{-ba}{ad-bc} \\
    0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}
  \end{pmatrix} \\
  \simeq & 
  \begin{pmatrix} 1 & 0 & \frac{d}{ad-bc} & \frac{-b}{ad-bc} \\
    0 & 1 & -\frac{c}{ad-bc} & \frac{a}{ad-bc}.
  \end{pmatrix}
\end{align*}
So, $A^{-1} = \begin{pmatrix} \frac{d}{ad-bc} & \frac{-b}{ad-bc} \\
  -\frac{c}{ad-bc} & \frac{a}{ad-bc} \end{pmatrix}$. For this to
exist, we must have $ad - bc \neq 0$. (The steps to get the formula
for $A^{-1}$ divided by $a$, so we also assumed $a \neq 0$. However,
if $a = 0$, we could start Gaussian elimination by swapping the first
and second rows, divide by $c$, and end up with the same formula. If
$c=0$ as well, then $ad - bc = 0$ too, so $ad - bc \neq 0$ is really
the only condition needed). 

This calculation suggests that $ad - bc $ is a good candidate for the
determinant of $2$ by $2$ matrices. You may remember from the review
problem set that $|ad-bc|$ is the area of the parallelogram
constructed from the columns of $A$. A nice interpretation of this
parallelogram is that it is the image of the unit square under the
linear transformation represented by $A$. Additionally, if $\det A >
0$, the transformation only stretches and shrinks the unit squares. If
$\det A<0$, the transformation also reflects the unit square over the
line $x = -y$. Similarly, in higher dimensions you can consider the
image of the unit cube (in 3d) or unit hypercube (i.e.\ higher
dimensional cube). This image will be a higher-dimension analog of a
parallelogram, and you could calculate its volume. It will turn out
that $|\det A|$ is the volume of the hyper-parallelogram.\footnote{Not
  a standard name.}
\begin{figure} \caption{Transformed unit square and unit cube}
  \begin{tabular}{cc}
    \includegraphics[width=0.45\linewidth]{det2} &
    \includegraphics[width=0.45\linewidth]{det3} 
  \end{tabular}
\end{figure}

\begin{definition}\label{deta}
  Let $A$ be an $n$ by $n$ matrix consisting of column vectors $a_1,
  ..., a_n$. The determinant of $A$ is the unique function such that
  \begin{enumerate}
  \item\label{d1} $\det I_n = 1$.
  \item\label{d2} As a function of the columnes, $\det$ is an
    alternating form: 
    $\det (A) = 0$ iff $a_1, ..., a_n$ are linearly dependent.
  \item\label{d3} As a function of the columnes, $\det$ is multi-linear:
    \[
    \det(a_1, ..., b a_j + c v, ..., a_n) = b\det(A) +
    c\det(a_1,...,v,...a_n) 
    \]
  \end{enumerate}
\end{definition}
Condition (\ref{d1}) seems very natural. Also, if we want $\det A$ to
be the volume of the transformed unit cube, we must have $\det I_n =
1$. (\ref{d2}) ensures that $\det A = 0$ whenever $A$ is not
invertible. 
\begin{lemma}
  Let $A$ be an $n$ by $n$ matrix. The $A$ is singular if and only if
  the columns of $A$ are linearly dependent.
\end{lemma}
\begin{proof}
  If $A$ is singular, then $\rank A < n$. Also $\rank A^T = \rank A$. 
  Therefore there must exist row operations such that the last row of
  $A^T$ is all $0$'s. Row operations are interchanging rows, rescaling
  by a scalar, and adding multiples of one row to another. Therefore,
  there must be $c_1, ..., c_n \in \R$ such that
  \[ c_1 a_1 + ... + c_n a_n = 0 \]
  where $a_i$ are the rows of $A^T$, which are also the columns of
  $A$.

  Similarly if the columns of $A$ are linearly dependent, then there
  are $c_1, ..., c_n \in \R$ such that
  \[ c_1 a_1 + ... + c_n a_n = 0. \]
  These are all row operations, so we can perform them to make the
  last row of $A^T$ all $0$'s. We can then perform Gaussian
  elimination on the first $n-1$ rows of $A^T$ to put it into row
  echelon form with the last row all $0$'s. Therefore, $\rank A<n$ and
  $A$ is singular.
\end{proof}
\begin{corollary}\label{cor:dns}
  $A$ is nonsingular if and only if $\det A \neq 0$.
\end{corollary}

The third condition on the determinant, \ref{d3}, guarantee that $\det
A$ can be interpreted as the volume of the transformed unit cube. It
is easiest to see this by thinking about a diagonal matrix. If $A$ is
diagonal with elements $a_{11}, a_{22}, ..., a_{nn}$, then volume of
the transformed unit cube will be a higher dimensional rectangle and
its volume will be $|\det A| = |\prod_{i=1}^n a_{ii} |$. If we
multiply any of the columns of $A$ by $c$, we will scale one of the
corners of the rectangle by $c$, so the volume should be multiplied by
$c$. Similarly, if we add $v$ to the $j$ diagonal element, the
volume will be 
\begin{align*}
  |a_{11}\times ...\times (a_{jj}+v)\times ... \times a_{nn} | = &
  |\prod_{i=1}^n a_{ii} | +  |a_{11}\times ...\times v \times
  ... \times a_{nn} | 
\end{align*}
So, at least for diagonal matrices, if $|\det A| $ is to be the volume
of the transformed unit cube, it must be multilinear. It turns out
that this is true in general as well.

Okay, so we have defined the determinant in a way that has a nice
interpretation, but how can we calculate it? There is an explicit
formula for the determinant, but it is not so simple.  
\begin{definition}\label{detc}
  The \textbf{determinant} of a square matrix $A$ is defined
  recursively as
  \begin{enumerate}
  \item For $1$ by $1$ matrices, $\det A = a_{11}$
  \item For $n$ by $n$ matrices, 
    \[ \det A = \sum_{j=1}^n (-1)^{1+j} a_{1j} \det A_{-1,-j} \]
    where $A_{-i,-j}$ is the $n-1$ by $n-1$ matrix obtained by
    deleting the $i$th row and $j$th column of $A$.
  \end{enumerate}
\end{definition}
$\det A_{-i,-j}$ is called a \textbf{minor} of $A$. $(-1)^{i+j}\det
A_{-i,-j}$ is called a \textbf{cofactor} of $A$. The above definition
is sometimes called the expansion by cofactors of the determinant. If
you are more comfortable with this definition than with the abstract
definition (\ref{deta}) above, you can consider this formula to be the
definition of the determinant. The two defitions are the same, as
stated in the following theorem, which we will not prove.
\begin{theorem}
  The two definitions of the determinant, (\ref{deta}) and
  (\ref{detc}), are equivalent.
\end{theorem}
In some situations, the abstract definition (\ref{deta}) is more
useful. Proving corollary \ref{cor:dns} from the abstract definition
was not difficult. Proving it using the recursive definition
(\ref{detc}) is a bit tedious. On the other hand, it will be easier to
prove part of lemma \ref{lem:detprop} using the recursive definition
than the abstract definition.

Some useful properties of determinants are stated in the following lemma.
\begin{lemma}\label{lem:detprop}
  Let $A$ and $B$ be square matrices then
  \begin{enumerate}
  \item $\det A^T = \det A$
  \item $\det (AB) = (\det A) (\det B)$
  \item $\det A^{-1} = (\det A)^{-1}$
  \item Usually, $\det(A + B) \neq \det A + \det B$
  \item If $A$ is diagonal, $\det A = \prod_{i=1}^n a_{ii}$
  \item\label{dpt} If $A$ is upper or lower triangular $\det A = \prod_{i=1}^n
    a_{ii}$.  
  \end{enumerate}
\end{lemma}
\begin{proof}
  The proof of all these properties would be quite long, so we will
  only prove \ref{dpt}. We can just prove it for lower triangular
  matrices, since $\det A^T = \det A$ and the transpose of upper
  triangular matrices is lower triangular. We will use induction on
  the size of $A$. If $A$ is $1$ by $1$, the claim is true. If $A$ is
  $n$ by $n$, from definition \ref{detc},
  we have
  \[ \det A = \sum_{j=1}^n (-1)^{1+j} a_{1j} \det A_{-1,-j}. \]
  $A$ is lower triangular, so only the first term in the sum is
  nonzero.
  \[ \det A = a_{11} \det A_{-1,-1} \]
  $A_{-1,-1}$ is also be lower triangular and is size $n-1$ by $n-1$,
  so by induction, $\det A = \prod_{i=1}^n a_{ii}$. 
\end{proof}

We can use the determinant to solve systems of linear equations and to
calculate $A^{-1}$. 
\begin{theorem}
  Let $A$ be nonsingular. Then,
  \begin{enumerate}
  \item $A^{-1} = \frac{1}{\det A} \begin{pmatrix} 
      \det A_{-1,-1} & \cdots & (-1)^{1+n} \det A_{-n,-1} \\
      \vdots & \ddots & \vdots \\
      (-1)^{1+n} \det A_{-1,-n} & \cdots & (-1)^{n+n} \det A_{-n,-n} 
    \end{pmatrix}$
  \item (\textbf{Cramer's rule}) The unique solution to $A x = b$ is 
    \[ x_i = \frac{\det B_i}{\det A} \]
    where $B_i$ is the matrix $A$ with the $i$th column replaced by
    $b$. 
  \end{enumerate}
\end{theorem}
This theorem can sometimes be useful for analyzing small systems of
linear equations and doing comparative statics. For an example, see
Simon and Blume 9.3. However, it is a very inefficient way to compute
$A^{-1}$ or solve a system of equations. 

\section{Computational efficiency}
Last class, someone asked about whether there was a more efficient way
to calculate the rank of a matrix than Gaussian elimination. The way
we usually compare the efficiency of algorithms is by looking at the
rate at which number of steps required grows as the size of the
problem increases. As an example, we will compare solving a system of
equations by Gaussian elimination to using Cramer's rule.

Let's start by considering computing the determinant
recursively as in definition \ref{detc}. Let $d(n)$ be the number of
steps required to compute the determinant for an $n$ by $n$
matrix. Given the recursive definition, $d(n)$ must satisfy
\begin{align*}
  d(n) = n d(n-1) + 2n 
\end{align*}
since $\det A_{-1,-i}$ must be computed $n$ times, and there are $n$
additions and multiplications. We could show by induction that
\begin{align*}
  d(n) = 2n! \sum_{k=1}^n \frac{1}{(n-k)!} .
\end{align*}
When comparing algorithms, we often mainly care about the number of
steps for large $n$. We say $d(n)$ is on the order of $f(n)$ or $d(n)$
is big O of $f(n)$ and write $d(n) = O(f(n))$ if $\exists n_0$ such
that
\[
  d(n) \leq M f(n) 
\]
for some constant $M$ and all $n \geq n_0$. From the above, $d(n) =
O(n!)$. Factorial grows very fast, so the determinant is quite
expensive to compute using the recursive definition for large
matrices.\footnote{Any reasonable computer program will not be
  computing determinants this way. It will compute some matrix
  decomposition (LU, QR, or Cholesky, or less likely, SVD) and then
  use that to compute the determinant.} To actually solve a system of
equations using Cramer's rule, we would have to compute $n+1$
determinants, so Cramer's rule is $O((n+1)!)$.

Now lets look at Gaussian elimination. Let $g(n)$ be the maximal
number of steps Gaussian elimination takes on an $n$ by $n$ matrix. In
the worst case, Gaussian elimination on the first row involves multiplying
the first row by $n-1$ numbers, so there are $n(n-1)$ multiplications,
and adding it to each of the $n-1$ rows, so there are $n(n-1)$
additions. The total number of steps to get to the next $n-1$ by $n-1$
submatrix is then $2n(n-1)$. Hence, $g(n)$ is given by
\[ g(n) = 2n(n-1) + g(n-1) \]
and,
\begin{align*}
  g(n) = & 2 \sum_{k=1}^n k(k-1) \\
  = & \frac{2}{3} (n^3 - n)
\end{align*}
Therefore, $g(n) = O(n^3)$. To solve a system of equations, we also
have to back substitute. The first substitution involes one division,
and then $n-1$ additions. The second involves one division and $n-2$
additions. In total there will be $\sum_{k=1}^n k = \frac{1}{2}
n(n-1)$ steps. So solving a system by Gaussian elimination takes 
$\frac{2}{3} (n^3 - n) + \frac{1}{2}(n^2 - n) = O(n^3)$ steps. It is
much faster than Cramer's rule. 

\section{Matrix decompositions}

There are number of matrix decompositions that are useful for
computing solutions to systems of linear equations, inverting
matrices, and computing determinants. 

\begin{definition}
  The \textbf{LU decomposition} of a square matrix is a decomposition
  of the form
  \[ P A = L U \]
  where $P$ is a permutation matrix, $L$ is lower triangular, and $U$
  is upper triangular. 
\end{definition}
We already know how to compute an LU decomposition by performing
Gaussian elimination. All row interchange operations can be
represented by the permutation matrix $P$. Adding multiples of one row to
another are represented by $L$, and the resulting row echelon form is
$U$. The LU decomposition exists for any nonsingular matrix. Sometimes
it is also required that the diagonal of $L$ by all $1$'s. In that
case, the LU decomposition is unique. The LU decomposition is used to
solve linear equations, compute $A^{-1}$, and compute $\det A$.  

\begin{definition}
  The \textbf{Cholesky decomposition} of a square, symmetric, and
  positive definite ($\det A > 0$) matrix $A$ is
  \[ A = L L^T \] 
  where $L$ is lower triangular with positive entries on the
  diagonal. 
\end{definition}
The Cholesky decomposition takes half as many steps to compute than
the LU decomposition and can be used for all the same
purposes. Symmetric positive definite matrices are quite common in
economics. Examples include covariance matrices and the matrix of
second derivatives of a convex minimization problem. 

\begin{definition}
  The \textbf{QR decomposition} of an $m$ by $n$ matrix is 
  \[ A = QR \]
  where $R$ is $m$ by $n$ and upper triangular and $Q$ is $m$ by $m$
  and orthogonal (i.e.\ $Q^T = Q^{-1}$).
\end{definition}
The QR decomposition can be used for all the things the LU
decomposition can be used for. The QR decomposition takes twice as
many steps to compute, but can be computed more accurately (which can
matter for very large matrices, but usually isn't important for small
ones).  The QR decomposition is also used for solving least square
problems. We will (likely) learn how to calculate the QR decomposition
and learn an interpretation of $Q$ next lecture. 

\begin{definition}
  The \textbf{singular value decomposition} of a matrix $m$ by $n$
  matrix $A$ is
  \[ A = U \Sigma V \]
  where $U$ is $m$ by $m$ and orthogonal, $\Sigma$ is $m$ by $n$ and
  diagonal, and $V$ is $n$ by $n$ and orthogonal.
\end{definition}
We will study the singular value decomposition in more detail next
week. 


\end{document}
