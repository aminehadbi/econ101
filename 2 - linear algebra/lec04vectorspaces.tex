\documentclass[12pt,reqno]{amsart}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage[left=1in,right=1in,top=0.9in,bottom=0.9in]{geometry}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{fancyhdr}
%\usepackage[small,compact]{titlesec} 

%\usepackage{pxfonts}
%\usepackage{isomath}
\usepackage{mathpazo}
%\usepackage{arev} %     (Arev/Vera Sans)
%\usepackage{eulervm} %_   (Euler Math)
%\usepackage{fixmath} %  (Computer Modern)
%\usepackage{hvmath} %_   (HV-Math/Helvetica)
%\usepackage{tmmath} %_   (TM-Math/Times)
%\usepackage{cmbright}
%\usepackage{ccfonts} \usepackage[T1]{fontenc}
%\usepackage[garamond]{mathdesign}
\usepackage{color}
\usepackage{ulem}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{assumption}{}[section]
%\renewcommand{\theassumption}{C\arabic{assumption}}
\newtheorem{definition}{Definition}[section]
\newtheorem{step}{Step}[section]
\newtheorem{remark}{Comment}[section]
\newtheorem{example}{Example}[section]
\newtheorem*{example*}{Example}

\linespread{1.1}

\pagestyle{fancy}
%\renewcommand{\sectionmark}[1]{\markright{#1}{}}
\fancyhead{}
\fancyfoot{} 
%\fancyhead[LE,LO]{\tiny{\thepage}}
\fancyhead[CE,CO]{\tiny{\rightmark}}
\fancyfoot[C]{\small{\thepage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\fancypagestyle{plain}{%
\fancyhf{} % clear all header and footer fields
\fancyfoot[C]{\small{\thepage}} % except the center
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}}

\makeatletter
\renewcommand{\@maketitle}{
  \null 
  \begin{center}%
    \rule{\linewidth}{1pt} 
    {\Large \textbf{\textsc{\@title}}} \par
    {\normalsize \textsc{Paul Schrimpf}} \par
    {\normalsize \textsc{\@date}} \par
    {\small \textsc{University of British Columbia}} \par
    {\small \textsc{Economics 526}} \par
    \rule{\linewidth}{1pt} 
  \end{center}%
  \par \vskip 0.9em
}
\makeatother

\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\def\inprobLOW{\rightarrow_p}
\def\inprobHIGH{\,{\buildrel p \over \rightarrow}\,} 
\def\inprob{\,{\inprobHIGH}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
\def\F{\mathbb{F}}
\def\R{\mathbb{R}}
\newcommand{\gmatrix}[1]{\begin{pmatrix} {#1}_{11} & \cdots &
    {#1}_{1n} \\ \vdots & \ddots & \vdots \\ {#1}_{m1} & \cdots &
    {#1}_{mn} \end{pmatrix}}
\newcommand{\iprod}[2]{\left\langle {#1} , {#2} \right\rangle}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\renewcommand{\det}{\mathrm{det}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\spn}{\mathrm{span}}
\newcommand{\row}{\mathrm{Row}}
\newcommand{\col}{\mathrm{Col}}
\renewcommand{\dim}{\mathrm{dim}}
\newcommand{\prefeq}{\succeq}
\newcommand{\pref}{\succ}

\title{Vector spaces}
\date{\today}

\begin{document}

\maketitle

The last lecture introduced vector spaces. In this lecture we will
explore vector spaces in more detail, which will eventually let us
complete our characterization of the set of solutions to systems of
linear equations. 

Remember that our the vector space with which we are most interested
is Euclidean space, $\R^n$. In fact, a good way to think about other
vector spaces is that they are just variations of $\R^n$. The whole
reason for defining and studying abstract vector spaces is to take our
intuitive understanding of two and three dimensional Euclidean space
and apply it to other contexts. If you find the discussion of abstract
vector spaces and their variations to be confusing, you can ignore it
and think of two or three dimensional Euclidean space instead. For the
exams in this course, I am unlikely to ask about vector spaces
other than $\R^n$. It is likely that you will not need to know
anything about vector spaces other than $\R^n$ throughout the masters
program. However, if you read enough articles in economics journals,
you will come across abstract vector spaces, and hopefully what we
have covered in this course will be helpful. Also, if you plan to
continue and get a PhD it will be useful to know about abstract vector
spaces.

\section{Normed vector spaces}

One property of two and three dimensional Euclidean space is that
vectors have lengths. Our definition of vector spaces from last
lecture does not guarantee that we have a way of measuring length in
all vector spaces, so let's define a special type of vector space
where we can measure length.
\begin{definition}
  A \textbf{normed vector space}, $(V,\F,+,\cdot, \norm{\cdot})$, is a
  vector space with a function, called the \textbf{norm}, from $V$ to
  $\F$ and denoted by $\norm{v}$ with the following properties:
  \begin{enumerate}
  \item $\norm{v} \geq 0$ and $\norm{v} = 0$ iff $v = 0$,
  \item $\norm{\alpha v} = |\alpha|\norm{v}$ for all $\alpha \in \F$,
  \item The triangle inequality holds:
    \[ \norm{v_1+v_2} \leq \norm{v_1} + \norm{v_2} \]
    for all $v_1, v_2 \in V$.
  \end{enumerate}
\end{definition}
As in the previous lecture, when the field, addition, multiplication,
and norm are clear from context, we will just write $V$ instead of
$(V,\F,+,\cdot, \norm{\cdot})$ to denote a normed vector space. 
Like length, a norm is always non-negative and only zero for the zero
vector. Also, similar to length, if we multiply a vector by a scalar,
the norm also gets multiplied by the scalar. The triangular inequality
means that norm obeys the idea that the shortest distance between two
points is a straight line. If you go directly from $x$ to $y$ you
``travel'' $\norm{x - y}$. If you stop at point $z$ in between, you travel
$\norm{x - z} + \norm{z - y}$. The triangle inequality guarantees that 
\[ \norm{x-y} \leq \norm{x - z} + \norm{z - y}. \]

\subsection{Examples}
\begin{example}
  $\R^3$ is a normed vector space with norm
  \[ \norm{x} = \sqrt{x_1^2 + x_2^2 + x_3^2}. \] 
  This norm is exactly
  how we usually measure distance. For this reason, it is called the
  Euclidean norm.

  More generally, for any $n$, $\R^n$, is a normed vector space with
  norm 
  \[ \norm{x} = \sqrt{\sum_{i=1}^n x_i^2 }. \]
\end{example}
The Euclidean norm is the most natural way of measuring distance in
$\R^n$, but it is not the only one.  A vector space can often be given
more than one norm, as the following example shows.
\begin{example}
  $\R^n$ with the norm 
  \[ \norm{x}_p = \left( \sum_{i=1}^p |x_i|^p \right)^{1/p} \]
  for $p \in [1,\infty]$\footnote{Where $\norm{x}_\infty = \max_{1\leq
      i \leq n} |x_i| $} is a normed vector space. This norm is called
  the p-norm. 
\end{example}
For nearly all practical purposes, $\R^n$ with any p-norm is
essentially the same as $\R^n$ with any other p-norm. $\R^n$ is the
same collection of elements regardless of the choice of p-norm, and
the choice of p-norm does not affect the topology of $\R^n$ or the
definition of derivatives.\footnote{We will discuss topology next
  lecture, and derivatives soon afterward, so do not worry if you do
  not know what that means.} However, there are normed vector spaces
where the choice of norm makes a difference.
\begin{example}
  $\mathcal{L}^p(0,1)$ with p-norm 
  \[ \norm{f}_p = \left(\int_0^1 |f(x)|^p dx\right)^{1/p} \] is a
  normed vector space. Moreover, $\mathcal{L}^p(0,1)$ is a different
  space for different $p$. For example, $\frac{1}{x^{1/p}} \not\in
  \mathcal{L}^p(0,1)$, but $\frac{1}{x^{1/p}} \in \mathcal{L}^q(0,1)$
  for $q < p$. 
\end{example}

\section{Inner product spaces}

Another example of a normed vector space is any inner product space.
Recall the definition of an inner product space from last lecture.
\begin{definition}
  A real \textbf{inner product space} is a vector space over the field
  $\R$ with an additional operation called the inner product that is
  function from $V \times V$ to $\mathbb{R}$. We denote the inner
  product of $v_1, v_2 \in V$ by $\iprod{v_1}{v_2}$. It has the
  following properties:
  \begin{enumerate}
  \item Symmetry: $\iprod{v_1}{v_2} = \iprod{v_2}{v_1}$
  \item Linear: $\iprod{a v_1 + b v_2}{v_3} = a \iprod{v_1}{v_3} + b
    \iprod{v_2}{v_3}$ for $a, b \in \R$
  \item Positive definite: $\iprod{v}{v} \geq 0$ and equals $0$ iff
    $v=0$. 
  \end{enumerate}  
\end{definition}
Any inner product space is also a normed vector space with norm
\[ \norm{x} = \sqrt{\iprod{x}{x}}. \]
Recall from the previous lecture that the inner product on $\R^n$ is
the dot product,
\[ \iprod{x}{y} = x\cdot y = \sum_{i=1}^n x_i y_i. \] 
The norm induced by the inner product is then
\[ \norm{x} = \sqrt{\iprod{x}{x}} = \sqrt{\sum_{i=1}^n x_i^2 }, \]
which is the usual Euclidean norm. Henceforth, whenever talking about
inner product spaces we will use $\norm{x}$ to denote the norm induced
by the inner product (which is the same as the 2-norm or Euclidean
norm).

Inner product spaces are special in another way. Remember that we are
studying vector spaces and their variants to try to generalize our
understand of three dimensional Euclidean space to other
contexts. Vector spaces are places where we can add elements and
multiply by scalars, just like in 3-d Euclidean space. In normed
vector spaces, we can also measure distance. Another thing that we
know how to do in Euclidean space is measure angles. Inner product
spaces are vector spaces where we can also measure angles. 

Suppose we have an inner product space then:
\begin{align*}
  \norm{x + y}^2 = & \iprod{x+y}{x+y} \\
  = & \iprod{x}{x} + 2\iprod{x}{y} + \iprod{y}{y}
\end{align*}
In $\R^n$ with the Euclidean norm when $x$ and $y$ are at right angles
to one other, $\iprod{x}{y} = 0$, and we have the Pythagorean theorem:
\[ \norm{x + y}^2 = \norm{x}^2 + \norm{y}^2. \]
This motivates the following definition
\begin{definition}
  Let $x,y \in V$, an inner product space. $x$ and $y$ are
  \textbf{orthogonal} iff $\iprod{x}{y} = 0$. 
\end{definition}

In Euclidean space, the inner product and angle are related by the
following theorem,
\begin{theorem}\label{thm:angle}
  Let $u, v \in \R^n$, then the angle between them is
  \[ \theta = \cos^{-1} \frac{\iprod{u}{v}}{\norm{u}\norm{v}}. \]  
\end{theorem}
\begin{proof}
  We can prove this using the definition of the cosine and the
  Pythagorean theorem.  See Blume and Simon theorem 10.3 for
  pictures. Imagine $u$ and $v$ in $\R^2$. Form a right-angle triangle
  by drawing a line orthogonal to the line from the origin to $v$ and
  passing through $u$. Let $t v$, with $t \in \R$, be the point at the
  right-angle of the triangle. By definition, cosine is the ratio of
  the length of the adjacent side to the hypotenuse, which is
  \[ \cos \theta = \frac{\norm{tv}}{\norm{u}}. \]
  Now we just need to relate $\norm{tv}$ to $\iprod{u}{v}$. The
  opposite side of the triangle is length $\norm{u - tv}$, so by the
  Pythagorean theorem,
  \begin{align*}
    \norm{u}^2 = & \norm{tv}^2 + \norm{u - tv}^2 \\
    \norm{u}^2 = & t^2 \norm{v}^2 + \norm{u}^2 - 2t\iprod{u}{v} + t^2
    \norm{v}^2 
    \\
    2 t \iprod{u}{v} = & 2 t^2 \norm{v}^2 \\
    t = & \frac{\iprod{u}{v}}{\norm{v}^2}
  \end{align*}
  Plugging this result into the previous equation gives the conclusion.
\end{proof}
In inner product spaces other than $\R^n$, we could define angles to
fit theorem \ref{thm:angle}. For example, this would allow us to talk
about the angle between two functions in $\mathcal{L}^2(0,1)$. We will
not use this definition of angle very much though. The really
important thing to remember about inner product spaces and angles is
that vectors can be orthogonal and the Pythagorean theorem holds.

\subsection{Useful inequalities}
When we start looking at limits next week, will we often need to prove
that the norm of something is small. There are a number of
inequalities that we will repeatedly use. The most common is the
triangle inequality, which was part of our definition of norms.  The
triangle inequality has many implications, some of which are not
obvious. These implications are often useful in proofs. The most
common is what is known as the reverse triangle inequality.
\begin{theorem}[Reverse triangle inequality]
  Let $V$ be a normed vector space and $x,y \in V$. Then
  \[ \left| \norm{x} - \norm{y} \right| \leq \norm{x-y}. \]
\end{theorem}
\begin{proof}
  By the usual triangle inequality,
  \begin{align*}
    \norm{x} + \norm{x-y} \geq & \norm{y} \\
    \norm{x-y} \geq \norm{y} - \norm{x}
  \end{align*}
  and
  \begin{align*}
    \norm{y} + \norm{y-x} \geq & \norm{x} \\
    \norm{y-x} \geq \norm{x} - \norm{y}.
  \end{align*}
  Also, $\norm{y-x} = \norm{(-1)(x-y)} = |-1| \norm{x-y} =
  \norm{x-y}$ is greater than both $\norm{x}-\norm{y}$ and
  $\norm{y}-\norm{x}$ and 
  \[ \norm{x-y} \geq \left| \norm{x} - \norm{y} \right|. \]
\end{proof}

After the triangle inequality, arguably the most important inequality
in mathematics is the Cauchy-Schwarz inequality. 
\begin{theorem}[Cauchy-Schwarz inequality \label{thm:cauchy-schwarz}]
  Let $V$ be an inner product space and let $u,v\in V$. Then,
  \[ \left\vert \iprod{u}{v} \right\vert \leq \norm{u}\norm{v}. \]
\end{theorem}
\begin{proof}
  The idea of this proof can be illustrated in $\R^2$ by forming a right
  angle triangle with vertices at $0$, $u$, and $tv$ and sides of
  lengths $\norm{tv}$, $\norm{u-tv}$, and $\norm{v}$, where $t$ is
  chosen such that $v$ and $u-tv$ are orthogonal, as shown in the
  following diagram.
  \begin{center}
    \includegraphics[width=0.5\linewidth]{cauchy-schwarz.png}
  \end{center}
  We are choosing $t$ so that $\iprod{v}{u-tv} = 0$, so solving for $t$,
  \begin{align*}
    \iprod{v}{u-tv} = & 0 \\
    \iprod{v}{v} = & t \iprod{u}{v} \\
    t = & \frac{\iprod{u}{v}}{\norm{v}^2}.
  \end{align*}
  We can choose such a $t$ in any inner product space, not just
  $\R^2$.
  
  Now, let $z = u-tv$. By construction, $\iprod{z}{v} = 0$ and $u = tv
  + z$. Hence, by the Pythagorean theorem,
  \begin{align*}
    \norm{u}^2 = & \norm{tv}^2 + \norm{z}^2 \\
    = & \frac{\iprod{u}{v}^2}{\norm{v}^2} + \norm{z}^2 
  \end{align*}
  $\norm{z}^2\geq 0$, so
  \begin{align*}
    \norm{u}^2 \geq & \frac{\iprod{u}{v}^2}{\norm{v}^2} \\
    \norm{u}\norm{v} \geq |\iprod{u}{v}|.
  \end{align*}
\end{proof}
Notice that in the proof, we also saw that  $\norm{u}\norm{v} =
|\iprod{u}{v}|$ if and only if $\norm{z} = 0$. $\norm{z}$ is zero
whenever $u$ and $v$ are linearly dependent i.e. $u = \alpha v$ where
$\alpha \in \R$. 

\section{Projections}
The mapping from $u$ to $tv$ that we saw in the proofs of theorem
\ref{thm:angle} and \ref{thm:cauchy-schwarz} is so common that it has
a name.
\begin{definition}
  Let $V$ be an inner product space and $x,y \in V$. The (orthogonal) 
  \textbf{projection} of $y$ onto $x$ is 
  \[ P_x y = \frac{\iprod{y}{x}}{\norm{x}^2} x. \]
  More generally, the projection of $y$ onto a finite set $\{x_1, x_2,
  ... , x_k\}$ is
  \[ P_{\{x_j\}_{j=1}^k} y = \sum_{j=1}^k P_{x_j - P_{\{x_i\}_{i \neq
        j}}} x_j y. \]
  Equivalently, 
  \[ P_{\{x_j\}_{j=1}^k} y = X (X^T X)^{-1} X^T y \]
  where $X$ as an $n$ by $k$ matrix whose columns are $x_k$. 
  
  More generally still, if $X \subseteq V$ is a linear subspace, then
  the projection of $y$ onto $X$ is
  \[ P_{X} y = P_{\{b_j\}_{j=1}^k} y \]
  where  $b_1, ..., b_k$ are an orthogonal basis for $X$. 
  
  Finally, if $Y \subseteq V$ the projection of $Y$ onto $X$ is just
  the set consisting of the projection of each element of $y$ onto
  $X$, i.e.
  \[ P_{X} Y = \{ P_{X} y : y \in Y \}. \]
\end{definition}
In $\R^2$ the projection of $y$ onto $x$ can be visualized by drawing
a line that passes through $y$ and is perpendicular to the line
connecting $x$ and the origin. In general, the projection of $y$
onto a subspace $X$ will be the point in $x$ that is closest to
$y$. This point will lie on a line that is orthogonal to $X$ and
passes through $y$. Projections are related to linear transformations
and matrices as well.
\begin{lemma}
  Any projection is an idempotent linear transformation. 
\end{lemma}
\begin{proof}
  First, we verify that projections have the two properties required
  for them to be linear transformations.
  \begin{enumerate}
  \item \begin{align*}
      P_x (y_1 + y_2) = & \frac{\iprod{x}{y_1+y_2}}{\norm{x}^2} x \\
      = & \frac{\iprod{x}{y_1}}{\norm{x}^2} x +
      \frac{\iprod{x}{y_2}}{\norm{x}^2}x   \\
      = & P_x y_1 + P_x y_2 
    \end{align*}
  \item \begin{align*}
      P_x(\alpha y) = & \frac{\iprod{x}{\alpha y}}{\norm{x}^2} x \\
      =& \alpha \frac{\iprod{x}{y}}{\norm{x}^2} x \\
      = & \alpha P_x y.
    \end{align*}
  \end{enumerate}
  Now, we show that projections are idempotent. 
  \begin{align*}
    P_x (P_x y) = & P_x\left(\frac{\iprod{x}{y}}{\norm{x}^2} x \right)
    \\
    = & \frac{\iprod{x}{\frac{\iprod{x}{y}}{\norm{x}^2} x}}{\norm{x}^2} x \\
    = & \frac{\iprod{x}{y}}{\norm{x}^2}
    \frac{\iprod{x}{x}}{\norm{x}^2} x \\
    = & P_x y.
  \end{align*}
\end{proof}
It turns out that any symmetric idempotent linear transformation can
be written in the form used to define (orthogonal)
projections. Therefore, projections are sometimes defined as
idempotent linear transformations instead. This definition is
equivalent to ours. We will not prove this, but we will have the tools
to prove it after this lecture so you might want to do it as an
exercise.

\section{Linear independence}

Recall the definition of linear independence from last lecture. 
\begin{definition}
  A set of vectors $v_1, ..., v_k \in V$, is \textbf{linearly
    independent} if the only solution to
  \begin{align*}
    \sum_{j=1}^k c_j v_j = 0 
  \end{align*}
  is $c_1 = c_2 = ... = c_k = 0$. 
\end{definition}

\subsection{Checking linear independence}
Given a set of vectors, $\mathbf{v}_1, ..., \mathbf{v}_n \in \R^m$ (or
any $n$-dimensional vector space), how do check whether they are
linearly independent? Well, by definition, they are linearly
independent if $c_1 = c_2 = ... = c_n = 0$ is the only solution to
\begin{align*}
  \sum_{j=1}^n c_j \mathbf{v}_j = 0 
\end{align*}
If we write this condition as a system of linear equations we have
\begin{align*}
  v_{11} c_1 + v_{12} c_2 + ... + v_{1n} c_n & = 0 \\
  \vdots & = \vdots \\
  v_{m1} c_1 + v_{m2} c_2 + ... + v_{mn} c_n & = 0 
\end{align*}
or in matrix form,
\begin{align*}
  \gmatrix{v} \begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} = &
  0 \\
  V \mathbf{c} = & 0 
\end{align*}
We call any system with $0$ on the right hand side a
\textbf{homogeneous} system. Any homogeneous system always has
$\mathbf{c}=0$ as a solution. We know from lecture 2 that it will have
other solutions if the rank of $V$ is less than $n$. This proves the
following lemma.
\begin{lemma}\label{lem:rankli}
  Vectors $\mathbf{v}_1, ..., \mathbf{v}_n \in \R^m$ are linearly
  independent if and only if
  \[ \rank(\mathbf{v}_1, ..., \mathbf{v}_n) = n. \]
\end{lemma}
\begin{corollary}\label{cor:kmli}
  Any set of $k>m$ vectors in $\R^m$ are linearly dependent.
\end{corollary}
For sets of $m$ vectors in $\R^m$ we can restate the lemma in terms of
the determinant.
\begin{corollary}\label{cor:detli}
  Vectors $\mathbf{v}_1, ..., \mathbf{v}_m \in \R^m$ are linearly
  independent if and only if
  \[ \det(\mathbf{v}_1, ..., \mathbf{v}_n) \neq 0. \]
\end{corollary}

\section{Row, column, and null space}

\subsection{Row space}
\begin{definition}
  Let $A$ be an $m$ by $n$ matrix. The \textbf{row space} of $A$,
  denoted $\row (A)$, is the space spanned by the row vectors of $A$.
\end{definition}
The row space of $A$ is a subspace of $\R^n$. 
\begin{lemma}
  Performing Gaussian elimination does not change the row space of a
  matrix. 
\end{lemma}
\begin{proof}
  Let $a_{1},...,a_m$ be the row vectors of $A$. Each step of Gaussian
  elimination transforms some $a_j$ into $a_j + g a_k$ with $k \neq j$
  or $g \neq -1$. It suffices to show that
  \[ \spn(a_1,...,a_m) = \spn(a_1 + g a_k , ..., a_m). \]
  If $ x \in \spn(a_1, ..., a_m)$, then 
  \begin{align*}
    x = & \sum_{i=}^m c_i a_i  \\
    = & c_1 (a_1 + g a_k) + (\sum_{i=2}^m c_i a_i) - c_1 g a_k,
  \end{align*}
  so $x \in \spn(a_1 + g a_k, ...., a_m )$. 

  Conversely if $ x \in \spn(a_1 + g a_k, ...., a_m )$, then
  \begin{align*}
    x = & c_1 (a_1 + g a_k) + \sum_{i=2}^m c_i a_i \\
    = & c_1 g a_k + \sum_{i=1}^m c_i a_i 
  \end{align*}
  so $x \in \spn(a_1,...,a_m)$.
\end{proof}
\begin{corollary}\label{cor:rankdimrow}
  The dimension of the row space of a matrix is equal to its rank.
\end{corollary}
\begin{proof}
  Let $r_1,..., r_m$ be the row vectors of the row echelon form of
  $A$. If the rank of $A$ is $k \leq m$, then $r_{k+1}=0, ...,
  r_{m}=0$. Also, for $j <= k$, $r_j$ has more leading zeros than
  $r_{j-1}$. A quick inductive argument then shows that $r_1,...,r_k$
  are linearly independent.  For $k = 1$, linear independence follows
  from $r_k \neq 0$. For $k >1$, assume $r_2,...,r_k$ are linearly
  independent. We cannot write $r_1 = (r_{11}, r_{12},..., r_{1n})$ as
  a combination of $r_2,...,r_k$ be they all begin with 0 and $r_{11}
  \neq 0$. Therefore $r_1,...,r_k$ are linearly independent, and $\dim
  \row(A) = \rank A$.  
\end{proof}

\subsection{Column space}

\begin{definition}
  Let $A$ be an $m$ by $n$ matrix. The \textbf{column space} of $A$,
  denoted $\col(A)$, is the space spanned by the column vectors of
  $A$.
\end{definition}
We know that the column space of an $n$ by $n$ matrix is $\R^n$ if and
only if the matrix is nonsingular. We will see that the column space
plays an important role in describing the set of solutions to systems
of linear equations.
\begin{lemma}
  Let $A$ be an $m$ by $n$ matrix. Then $A x = b$ has a solution iff $b
  \in \col(A)$.
\end{lemma}
\begin{proof}
  If $x$ solves $Ax = b$, then $b$ is in the column space of $A$ by
  definition. Conversely if $b$ is in the column space of $A$, then
  there must exist an $x$ such that $A x=  b$.
\end{proof}

Let us examine the dimension of the column space of $A$.
\begin{definition}
  A column of a matrix $A$ is \textbf{basic} if the corresponding
  column of the row echelon form, $A_r$, contains a pivot.
\end{definition}
\begin{theorem}
  The basic columns of $A$ form a basis for $\col(A)$. 
\end{theorem}
\begin{proof}
  Let $A$ be $m \times n$ and denote its columns as $v_1,...,v_n$. Let
  $A_r$ be the row echelon form of $A$ and denotes its columns as
  $w_1,...,w_n$. Let $w_{i_1}, ..., w_{i_k}$ be the basic columns of
  $A_r$. Then $w_{i_1}$ must end with more zeros than $w_{i_2}$, and
  $w_{i_2}$ must end with more zeros than $w_{i_3}$, etc. By the same
  argument as in corollary \ref{cor:rankdimrow}, $w_{i_1},...,w_{i_k}$
  are linearly independent. By definition of row echelon form, the
  final $m-k$ rows of $A_r$ are all zero. Therefore $\dim \col(A_r) \leq
  k$, and $w_{i_1},...,w_{i_k}$ must span $\col(A_r)$ and be a basis. 

  Now we must show that $v_{i_1}, ..., v_{i_k}$ are a basis for
  $\col(A)$. Suppose
  \[ c_1 v_{i_1} + ... + c_k v_{i_k} = 0. \]
  Then we could do Gaussian elimination to convert this system to
  \[ c_1 w_{i_1} + ... + c_k w_{i_k} = 0. \]
  $w_{i_1},... , w_{i_k}$ are linearly independent so $c_1 = 0
  ,... c_k = 0$ and $v_{i_1}, ..., v_{i_k}$ are linearly independent
  too. Moreover, if we add any other $v_j$, $j \not\in \{i_1, ..., i_k\}$,
  then by the same argument there must exist a non-zero $c$ than
  solves 
  \[ c_1 v_{i_1} + ... + c_k v_{i_k} + c_j v_j = 0. \]
  Thus, $v_{i_1} , ..., v_{i_k}$ is a basis for $\col(A)$. 
\end{proof}
\begin{corollary}
  The dimensions of the row and column spaces of any matrix are
  equal. 
\end{corollary}
\begin{proof}
  The previous proof showed that $\dim \col(A) = \rank A$, and earlier
  we showed that $\dim \row(A) = \rank A$. 
\end{proof}
This also proves that the ranks of $A$ and $A$ transpose are equal. We
stated this fact earlier without proof.
\begin{corollary}
  $\rank A = \rank A^T$.
\end{corollary}
When talking about linear transformations in general instead of just
matrices, the column space is called the image or range of the
transformation. All the results in this section still apply. 

\subsection{Null space}
\begin{definition}
  Let $A$ be $m$ by $n$. The set of solutions to the homogeneous
  equation $Ax = 0$ is the \textbf{null space} (or kernel) of
  $A$, denoted by $\mathcal{N}(A)$ (or $\mathrm{Null}(A)$).
\end{definition}

\begin{definition}
  Let $V \subseteq \R^n$ be a linear subspace, and let $c \in \R^n$ be
  a fixed vector. The set
  \[ \{ x \in \R^n: x = v + c \text{ for some } v \in V \} \]
  is called the set of \textbf{translates} of $V$ by $c$, and is
  denoted $c + V$. Any set of
  translates of a linear subspace is called an \textbf{affine} space. 
\end{definition}
Like linear subspaces, affine spaces are points, lines, planes, and
hyperplanes. Unlike linear subspaces, affine spaces do not necessarily
contain 0. 

\begin{lemma}
  Let $A x = b$ be an $m$ by $n$ system of linear equations. Let $x_0$
  be any particular solution. Then the set of solutions is $x_0 +
  \mathcal{N}(A)$. 
\end{lemma}
\begin{proof}
  Let $w \in x_0 + \mathcal{N}(A)$. Then 
  \begin{align*}
    Aw = & A(x_0) + A(\underbrace{w-x_0}_{\in \mathcal{N}(A)}) \\
    = & b + 0.
  \end{align*}
  Let $w$ be a solution to $Ax = b$. Then
  \begin{align*}
    A(w - x_0) = Aw - Ax_0 = 0
  \end{align*}
  so $w - x_0 \in \mathcal{N}(A)$ and $w \in x_0 + \mathcal{N}(A)$. 
\end{proof}

\begin{theorem}
  Let $A$ be an $m$ by $n$ matrix. Then 
  $\dim \mathcal{N}(A) = n - \rank A$
\end{theorem}
\begin{proof}
  Let $u_1, ..., u_k$ be a basis for $\mathcal{N}(A)$. If $k=n$, then
  $\dim \mathcal{N}(A) = n$ and $A x = 0$ for all $x \in \R^n$. It
  follows that $A$ must be all zeros so $\rank A = 0$ and the theorem
  is true. 

  If $k < n$, then we add $u_{k+1},...,u_n$ to $u_1, ..., u_k$ to form
  a basis for $\R^n$. We can do this because $u_1, ..., u_k$
  do not span $\R^n$, so there must be $u_{k+1} \not\in \spn(u_1, ...,
  u_k)$. This implies that $u_1, ..., u_k, u_{k+1}$ are linearly
  independent. Repeating this argument we can add $u_{k+2}, ...,
  u_n$. 

  Now we will show that  $A u_{k+1}, ..., A u_n$ are a basis for the
  column space. They are in the column space by definition. They are
  linearly independent because
  \begin{align*}
    c_{k+1} A u_{k+1} + ... + c_n A u_{n} = & A (c_{k+1} u_{k+1} +
    ... + c_n u_{n}) 
  \end{align*}
  $A (c_{k+1} u_{k+1} + ... + c_n u_{n})$ equals zero only if $ (c_{k+1} u_{k+1} +
  ... + c_n u_{n}) \in \mathcal{N}(A)$, but by construction this is
  only possible for $c_{k+1} =  ... = c_n = 0$.  

  Finally $A u_{k+1}, ..., A u_n$ span $\col A$ since if $A x = b$ we
  can write $x = c_1 u_1 + ... + c_n a _n$, and
  \begin{align*}
    b = A x = & A \left( c_1 u_1 + ... + c_n a_n \right) \\
    = & c_1 A \underbrace{u_1}_{\in \mathcal{N}(A)} + ... + c_k A
    \underbrace{u_k}_{\in \mathcal{N}(A)} + 
    c_{k+1}Au_{k+1} + ... +c_n A u_n \\
    b = & 0  +     c_{k+1}Au_{k+1} + ... +c_n A u_n 
  \end{align*}
  so $b \in \spn (A u_{k+1}, ..., A u_n )$. Thus, 
  \[ \dim \col(A) = n-k = n - \dim \mathcal{N}(A) = \rank A. \]
\end{proof}

Collecting all the above results, we have finally completed our
description of the set of all solutions to a system of linear
equations. 
\begin{theorem}[Rouch\'{e}-Capelli] \label{thm:rc} A system of linear
  equations with $n$ variables has a solution if and only if the rank
  of its coefficient matrix, $A$, is equal to the rank of its
  augmented matrix, $\hat{A}$. Equivalently, a solution exists if and
  only if $b \in \col(A)$.

  If a solution exists and $\rank A$ is equal to its number of
  columns, the solution is unique. If a solution exists and $\rank A$
  is less than its number of columns, there are infinite solutions. In
  this case the set of solutions forms is an affine space, $x_0 +
  \mathcal{N}(A)$, where $x_0$ is any particular solution to $A x =
  b$. This set of solutions is an affine subspace of dimension $n -
  \rank A$.
\end{theorem}

\subsection{Relationship among row, column, and null space}

Let's examine how the row, column, and null spaces of a matrix are
related. Suppose $A$ is $m$ by $n$, then $\col(A) \subseteq \R^m$,
$\row(A) \subseteq \R^n$ and $\mathcal{N}(A) \subseteq \R^n$. Given
that the row and null space are both subsets of $\R^n$, it is natural
to wonder how they are related. Since
the transpose just switches rows with columns, we know that $\row(A^T)
= \col(A)$ and $\col(A^T) = \row(A)$. Suppose $x \in \mathcal{N}(A)$,
then $A x = 0$. The definition of the transpose requires that for all
$w \in \R^m$, 
\begin{align*} 
  \iprod{A^T w}{x} = & \iprod{w}{Ax} \\
  \iprod{A^T w}{x} = & \iprod{w}{0} \\
  \iprod{A^T w}{x} = & 0
\end{align*}
But, the set $\{A^T w: w \in \R^m \}$ is $\col(A^T) = \row(A)$. Thus,
we can conclude that for any $x \in \mathcal{N}(A)$ and $y \in
\row(A)$, $\iprod{y}{x} = 0$. In other words, the row and null spaces
of a matrix are orthogonal. Similarly, $\mathcal{N}(A^T)$ and
$\row(A^T) = \col(A)$ are orthogonal. 

\end{document}
