\documentclass[compress]{beamer} 
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{verbatim}
%\usepackage[small,compact]{titlesec} 
%\usecolortheme{beaver}
\usetheme{Hannover}
%\usecolortheme{whale}

%\usepackage{pxfonts}
%\usepackage{isomath}
%\usepackage{mathpazo}
%\usepackage{arev} %     (Arev/Vera Sans)
%\usepackage{eulervm} %_   (Euler Math)
%\usepackage{fixmath} %  (Computer Modern)
%\usepackage{hvmath} %_   (HV-Math/Helvetica)
%\usepackage{tmmath} %_   (TM-Math/Times)
%\usepackage{tgheros}
%\usepackage{cmbright}
%\usepackage{ccfonts} \usepackage[T1]{fontenc}
%\usepackage[garamond]{mathdesign}
\usepackage{color}
\usepackage{ulem}

\setbeamertemplate{navigation symbols}{}
\AtBeginSection[] % Do nothing for \section*
{ \frame{\sectionpage} }

\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\def\inprobLOW{\rightarrow_p}
\def\inprobHIGH{\,{\buildrel p \over \rightarrow}\,} 
\def\inprob{\,{\inprobHIGH}\,} 
\def\indist{\,{\buildrel d \over \rightarrow}\,} 
\def\F{\mathbb{F}}
\def\R{\mathbb{R}}
\newcommand{\gmatrix}[1]{\begin{pmatrix} {#1}_{11} & \cdots &
    {#1}_{1n} \\ \vdots & \ddots & \vdots \\ {#1}_{m1} & \cdots &
    {#1}_{mn} \end{pmatrix}}
\newcommand{\iprod}[2]{\left\langle {#1} , {#2} \right\rangle}
\newcommand{\norm}[1]{\left\Vert {#1} \right\Vert}
\newcommand{\abs}[1]{\left\vert {#1} \right\vert}
\renewcommand{\det}{\mathrm{det}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\spn}{\mathrm{span}}
\newcommand{\row}{\mathrm{Row}}
\newcommand{\col}{\mathrm{Col}}
\renewcommand{\dim}{\mathrm{dim}}
\newcommand{\prefeq}{\succeq}
\newcommand{\pref}{\succ}
\newcommand{\seq}[1]{\{{#1}_n \}_{n=1}^\infty }
\renewcommand{\to}{{\rightarrow}}


\title{Differential Calculus}
\author{Paul Schrimpf}
\institute{UBC \\ Economics 526}
\date{\today}

\begin{document}

\frame{\titlepage}
%\setcounter{tocdepth}{2}

% \begin{frame}
%   \frametitle{Midterm}
%   \includegraphics[width=0.6\textwidth,angle=270]{midterm} \\

%   \begin{tabular}{c|ccc|ccc}
%     Question & 1 & 2 & 3 & 4 & 5 & 6 \\ \hline
%     N & 50 & 9 & 43  & 50 & 33 & 19\\
%     Average & 24 & 21 & 22 & 23 &  21 & 18 \\  \hline
%   \end{tabular}
% \end{frame}

\begin{frame}
  \tableofcontents  
\end{frame}

\section{Derivatives}

\subsection{Partial derivatives}
\begin{frame}
  \frametitle{Partial derivatives}
  \begin{definition}
    Let $f:\R^n \to R$. The $i$th \textbf{partial derivative} of $f$ is 
    \[ \frac{\partial f}{\partial x_i} (x_0) = \lim_{h \to 0}
    \frac{f(x_{01},...,x_{0i}, ... x_{0n}}{h}. \]
  \end{definition}
\end{frame}
\subsection{Examples}
\begin{frame}
\begin{example}
  Let $f:\R^n \to \R$ be a production function. Then we call
  $\frac{\partial f}{\partial x_i}$ the \textbf{marginal product} of
  $x_i$. If $f$ is Cobb-Douglas, $f(k,l) = Ak^\alpha l^\beta$, where
  $k$ is capital and $l$ is labor, then the marginal products of
  capital and labor are
  \begin{align*}
    \frac{\partial f}{\partial k} (k,l) = & A \alpha k^{\alpha-1}
    l^\beta \\
    \frac{\partial f}{\partial l} (k,l) = & A \beta k^{\alpha}
    l^{\beta -1}.
  \end{align*}
\end{example}
\end{frame}

\begin{frame}
\begin{example}
  If $u:\R^n \to \R$ is a utility function, then we call
  $\frac{\partial u}{\partial x_i}$ the marginal utility of $x_i$.  
  If $u$ is CRRA, 
  \[u(c_1,...,c_T) =
  \sum_{t=1}^T \beta^t \frac{c_t^{1-\gamma}}{1-\gamma} \]
  then  the marginal utility of consumption in period $t$ is 
  \[ \frac{\partial u}{\partial c_t} = \beta^t c_t^{-\gamma}. \]
\end{example}
\end{frame}

\begin{frame}
\begin{example}
  The price elasticity of demand is the percentage change in demand
  divided by the percentage change in its price. If $q_1:\R^3 \to \R$ is
  a demand function with three arguments: own price $p_1$, the price
  of another good, $p_2$, and consumer income, $y$.  The own price
  elasticity is 
  \[ \epsilon_{q_1,p_1} = \frac{\partial q_1}{\partial p_1}
  \frac{p_1}{q_1(p_1,p_2,y)}. \]
  The cross price elasticity is the percentage change in demand
  divided by the percentage change in the other good's price, i.e.
  \[ \epsilon_{q_1,p_2} = \frac{\partial q_1}{\partial p_2}
  \frac{p_2}{q_1(p_1,p_2,y)}. \]
  Similarly, the income elasticity of demand is
  \[ \epsilon_{q_1,y} = \frac{\partial q_1}{\partial y}
  \frac{y}{q_1(p_1,p_2,y)}. \]
\end{example}
\end{frame}

\subsection{Total derivatives}

\begin{frame}
  \includegraphics[width=\linewidth]{nondiff}
\end{frame}

\begin{frame}
  \frametitle{Total derivative}
  \begin{definition}
    Let $f: \R^n \to \R$. The \textbf{derivative} (or total derivative
    or differential) of $f$ at $x_0$ is a linear mapping, $Df_{x_0}:
    \R^n \to \R^1$ such that
    \begin{align*}
      \lim_{h \to 0} \frac{\left|f(x_0 + h) - f(x_0) - Df_{x_0} h\right|} {\norm{h}}
      = 0.
    \end{align*}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{theorem}\label{thm:tdiff}
    Let $f: \R^n \to \R$ be differentiable at $x_0$, then
    $\frac{\partial f}{\partial x_i}(x_0)$ exists for each $i$ and 
    \[ Df_{x_0} h = \begin{pmatrix} \frac{ \partial f}{\partial x_1}(x_0) &
      \cdots \frac{ \partial f}{\partial x_n }(x_0)
    \end{pmatrix} h. \]
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{proof}
    The definition of derivative says that
    \begin{align*}
      \lim_{t \to 0} \frac{\left|f(x_0 + e_i t) - f(x_0) - Df_{x_0} (e_i t)\right|}
      {\norm{e_i t} } & = 0 \\
      \lim_{t \to 0} \frac{f(x_0 + e_i t) - f(x_0) - t D f_{x_0} e_i}
      {|t| } & =  0
    \end{align*}
    This implies that
    \begin{align*}
      f(x_0 + e_i t) - f(x_0) = t D f_{x_0} e_i + r_i(x_0,t)
    \end{align*}
    with $\lim_{t \to 0} \frac{|r_i(x_0,t)|}{|t|} = 0$. Dividing by $t$, 
    \begin{align*}
      \frac{f(x_0 + e_i t) - f(x_0)}{t} = D f_{x_0} e_i + \frac{r_i(x_0,t)}{t}
    \end{align*}
    and taking the limit
    \begin{align*}
      \lim_{t \to 0} \frac{f(x_0 + e_i t) - f(x_0)}{t} = D f_{x_0} e_i 
    \end{align*}
  \end{proof}
\end{frame}

\begin{frame}
  \begin{theorem}\label{thm:ptdiff}
    Let $f:\R^n \to \R$ and suppose its partial derivatives exist and
    are continuous in $N_\delta(x_0)$ for some $\delta>0$. Then $f$
    is differentiable at $x_0$ with 
    \[ Df_{x_0}= \begin{pmatrix} \frac{ \partial f}{\partial x_1}(x_0) &
      \cdots \frac{ \partial f}{\partial x_n }(x_0)
    \end{pmatrix}. \]
  \end{theorem}
  A minor modification would show the stronger result that $f:\R^n \to
  \R$ has a continuous derivative on an open set $U \subseteq \R^n$ if
  and only if its partial derivatives are continuous on $U$. We call
  such a function \textbf{continuously differentiably} on $U$ and
  denote the set of all such function as $C^1(U)$.
\end{frame}

\subsection{Mean value theorem}

\begin{frame}
  \begin{theorem}
    Let $f:\R^n \to \R$ be continuous and $K \subset \R^n$ be
    compact. Then $\exists x^* \in K$ such that $f(x^*) \geq f(x)
    \forall x \in K$. 
  \end{theorem}
  \begin{definition}
    Let $f: \R^n \to \R$. we say that $f$ has a local maximum at $x$ if
    $\exists \delta > 0$ such that $f(y) \leq f(x)$ for all $y \in
    N_\delta(x)$. 
  \end{definition}
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $f: \R^n \to \R$ and suppose $f$ has a local maximum at $x$ and
    is differentiable at $x$. Then $Df_x = 0$. 
  \end{theorem}
  \begin{proof}
    \[ \frac{f(x+h) - f(x)}{\norm{h}} =\frac{ Df_x h +
      r(x,h)}{\norm{h}} \] where $\lim_{h \to 0}
    \frac{|r(x,h)|}{\norm{h}} = 0$. Let $h = t v$ for some $v \in \R^n$
    with $\norm{v} =1$ and $t \in \R$. If $D f_x v > 0$, then for $t>0$
    small enough, we would have $\frac{f(x+tv) - f(x)}{|t|} = D
    f_x v + \frac{r(x,tv)}{|t|} > D
    f_x v / 2 > 0$ and $f(x+tv)> f(x)$ in contradiction to $x$ being a
    local maximum. 
  \end{proof}
\end{frame}

\begin{frame}
  \frametitle{Mean value theorem}
  \begin{theorem}[mean value]\label{thm:mvt}
    Let $f:\R^n \to \R^1$ be in $C^1(U)$ for some open $U$. Let $x, y
    \in U$ be such that the line connecting $x$ and $y$, $\ell(x,y) =
    \{z\in \R^n: z = \lambda x + (1-\lambda) y, \lambda \in [0,1]\}$, is
    also in $U$. Then there is some $\bar{x} \in \ell(x,y)$ such that
    \[ f(x) - f(y) = Df_{\bar{x}} (x-y). \]
  \end{theorem}
  \begin{proof}
    Let $g(z) = f(y) - f(z) + \frac{f(x) - f(y)}{x-y} (z - y)$. Note
    that $g(x) = g(y) = 0$. The set $ell(x,y)$ is closed and bounded,
    so it is compact. Hence, $g(z)$ must attain its maximum on
    $\ell(x,y)$, say at $\bar{x}$, then the previous theorem shows
    that $Dg_{\bar{x}} = 0$. Simple calculation shows that
    \[ Dg_{\bar{x}} = -Df_{\bar{x}} +  \frac{f(x) - f(y)}{x-y} = 0 \]
    so 
    \[ Df_{\bar{x}}(x-y) = f(x) - f(y). \]
  \end{proof}
\end{frame}

\subsection{Functions from $\R^n \to \R^m$}

\begin{frame} \frametitle{Functions from $\R^n \to \R^m$}
  \begin{definition}
    Let $f: \R^n \to \R^m$. The \textbf{derivative} (or total derivative
    or differential) of $f$ at $x_0$ is a linear mapping, $Df_{x_0}:
    \R^n \to \R^m$ such that
    \begin{align*}
      \lim_{h \to 0} \frac{\left\Vert f(x_0 + h) - f(x_0) - Df_{x_0}
          h\right\Vert} {\norm{h}} = 0. 
    \end{align*}
  \end{definition}
  \begin{itemize}
  \item Theorems \ref{thm:tdiff} and \ref{thm:ptdiff} sill hold
  \item The total derivative of $f$ can be represented by the
    $m$ by $n$ matrix of partial derivatwives (the \textbf{Jacobian}),
    \[ Df_{x_0}  = \begin{pmatrix} \frac{\partial f_1}{\partial x_1}(x_0) &
      \cdots & \frac{\partial f_1}{\partial x_n}(x_0) \\
      \vdots & & \vdots \\
      \frac{\partial f_m}{\partial x_1}(x_0) & \cdots & \frac{\partial
        f_m}{\partial x_n}(x_0)  
    \end{pmatrix}. \] 
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{corollary}[mean value for $\R^n \to \R^m$]\label{thm:mvtm}
    Let $f:\R^n \to \R^m$ be in $C^1(U)$ for some open $U$. Let $x, y
    \in U$ be such that the line connecting $x$ and $y$, $\ell(x,y) =
    \{z\in \R^n: z = \lambda x + (1-\lambda) y, \lambda \in [0,1]\}$, is
    also in $U$. Then there are $\bar{x}_j \in \ell(x,y)$ such that
    \[ f_j(x) - f_j(y) = D{f_j}_{\bar{x}_j} (x-y) \]
    and
    \[ f(x) - f(y) = \begin{pmatrix} D{f_1}_{\bar{x}_1} \\
      \vdots \\
      D{f_m}_{\bar{x}_m} \end{pmatrix} (x-y). 
    \]  
  \end{corollary}
\end{frame}

\subsection{Chain rule}

\begin{frame}\frametitle{Chain rule}
  \begin{itemize}
  \item $f(g(x)) = f'(g(x)) g'(x)$. 
  \end{itemize}
  \begin{theorem} \label{thm:chain}
    Let $f:\R^n \to \R^m$ and $g: \R^k \to \R^n$. Let $g$ be
    continuously differentiable on some open set $U$ and $f$ be
    continuously differentiable on $g(U)$. Then $h:\R^k \to \R^m$, $h
    (x) = f(g(x))$ is continuously differentiable on $U$ with 
    \[ Dh_x = D f_{g(x)} D g_x \]
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{proof}
    Let $x \in U$. Consider
    \begin{align*}
      \frac{\norm{ f(g(x+d)) - f(g(x))}} {\norm{d}}.
    \end{align*}
    Since $g$ is differentiable by the mean value theorem, $g(x+d) =
    g(x) + Dg_{\bar{x}(d)} d$, so
    \begin{align*}
      \norm{ f(g(x+d)) - f(g(x))} = &  
      \norm{ f(g(x) + D g_{\bar{x}(d)} d ) - f(g(x))} \\
      \leq & \norm{f(g(x) + D g_x d) - f(g(x))} + \epsilon
    \end{align*}
  where the inequality follows from the the continuity of $D g_x$ and
  $f$, and holds for any $\epsilon >0$. $f$ is differentiable, so
  \[ \lim_{D g_x d \to 0} \frac{\norm{f(g(x) + D g_x d) -
      f(g(x)) - D f_{g(x)} D g_x d}} {\norm{D g_x d}} = 0 \]
  Using the Cauchy-Schwarz inequality, $\norm{D g_x d} \leq \norm{D
    g_x} \norm{d}$, so
  \[ \lim_{ d \to 0} \frac{\norm{f(g(x) + D g_x d) -
      f(g(x)) - D f_{g(x)} D g_x d}} {\norm{d}} = 0. \]   
\end{proof}
\end{frame}

\subsection{Higher order derivatives}
\begin{frame}\frametitle{Higher order derivatives}
  \begin{itemize}
  \item Take higher order derivatives of multivariate functions just
    like of univariate functions. 
  \item If $f: \R^n \to \R^m$, then is has $nm$
    partial first derivatives. Each of these has $n$ partial derivatives,
    so $f$ has $n^2m$ partial second derivatives, written
    $\frac{\partial^2 f_k}{\partial x_i \partial x_j}$. 
  \end{itemize}
\end{frame}
\begin{frame}
  \begin{theorem}
    Let $f: \R^n \to \R^m$ be twice continuously differentiable on some
    open set $U$. Then
    \[ \frac{\partial^2 f_k}{\partial x_i \partial
      x_j}(x) =  \frac{\partial^2 f_k}{\partial x_j \partial
      x_i} (x) \]
    for all $i,j,k$ and $ x \in U$.
  \end{theorem}
  \begin{corollary}
    Let $f: \R^n \to \R^m$ be $k$ times continuously differentiable on
    some open set $U$. Then 
    \[ \frac{\partial^k f}{\partial x_1^{j_1} \times \cdots
      \times \partial x_n^{j_n}} = 
    \frac{\partial^k f}{\partial x_{p(1)}^{j_{p(1)}} \times \cdots \times \partial
      x_{p(n)}^{j_{p(n)}}} \]
    where $\sum_{i=1}^n j_i = k$ and $p:\{1,..,n\} \to \{1,...,n\}$ is
    any permutation (i.e. reordering).
  \end{corollary}
\end{frame}

\subsection{Taylor series}
\begin{frame}\frametitle{Taylor series}
  \begin{theorem}[Univarite Taylor series]
    Let $f: \R \to \R$ be $k+1$ times continuously differentiable on some
    open set $U$, and let $a$, $a+h \in U$. Then 
    \[ f(a+h) = f(a) + f'(a) h + \frac{f^2(a)}{2} h^2 + ... +
    \frac{f^k(a)}{k!} h^k + \frac{f^{k+1}(\bar{a})}{(k+1)!} h^{k+1} 
    \]
    where $\bar{a}$ is between $a$ and $h$. 
  \end{theorem}
\end{frame}
\begin{frame}
  \begin{theorem}[Multivariate Taylor series]
    Let $f:\R^n \to \R^m$ be $k$ times continuously differentiable on
    some open set $U$ and $a, a+h \in U$. Then there exists a $k$ times
    continuously differentiable function $r_k(a,h)$ such that
    \[ f(a+h) = f(a) + \sum_{\sum_{i=1}^n {j_i}=1}^k\frac{1}{k!}
    \frac{\partial^{\sum j_i} f}{\partial x_{1}^{j_1} \cdots \partial
      x_{n}^{j_n}}(a) h_1^{j_1}h_2^{j_2} \cdots h_n^{j_n} +
  r_k(a,h) \]
  and $\lim_{h \to 0} \norm{r_k(a,h)}{\norm{h}^k} = 0$
  \end{theorem}
\end{frame}
\begin{frame}
  \begin{proof}
    Follows from the mean value theorem. For $k=1$, the mean value
    theorem says that
    \begin{align*}
      f(a+h) - f(a) = & Df_{\bar{a}} h \\
      f(a+h) = & f(a) + Df_{\bar{a}} h \\
      = & f(a) + Df_{a} h + \underbrace{(Df_{\bar{a}} -Df_a)h}_{r_1(a,h)}  
    \end{align*}
    $Df_a$ is continuous as a function of $a$, and as $h \to 0$,
    $\bar{a} \to a$, so $\lim_{h \to 0} r_1(a,h) = 0$, and the theorem
    is true for $k = 1$. For general $k$, suppose we have proven the
    theorem up to $k-1$. Then repeating the same argument with the
    $k-1$st derivative of $f$ in place of $f$ shows that theorem is true
    for $k$. 
  \end{proof}
\end{frame}

\section{Implicit functions}

\subsection{Inverse functions}
\begin{frame}\frametitle{Inverse functions}
  \begin{itemize}
  \item $f: \R^n \to \R^m$. 
  \item If $f(x) = y$, when can we solve
    for $x$ in terms of $y$? i.e. when is $f$ invertible? 
  \item Suppose $f(a) = b$, expand:
    \begin{align*}
      f(x) = f(a) + Df_a (x-a) + r_1(a,x-a)
    \end{align*}
    \begin{align*}
      f(a) + Df_a (x-a) & = y\\
      Df_a x = & y - f(a) + Df_a a
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame} 
  \begin{theorem}[Inverse function]
    Let $f: \R^n \to \R^n$ be continuously differentiable on an open set
    $E$. Let $a \in E$, $f(a) = b$, and $Df_a$ be invertible . Then 
    \begin{enumerate}
    \item there exist open sets $U$ and $V$ such that $a \in U$, $b \in
      V$, $f$ is one-to-one on $U$ and $f(U) = V$, and
    \item the inverse of $f$ exists and is continuously differentiable
      on $V$ with derivative $Df_{f^{-1}(x)}^{-1}$.
    \end{enumerate}
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{proof}
    Choose $\lambda$ such that $\lambda \norm{Df_a^{-1}} = 1/2$. Since
    $Df_a$ is continuous, there is an open neighborhood $U$ of $a$ such
    that $\norm{Df_x - Df_a} < \lambda$ for all $x \in U$. For any $y
    \in \R^n$, consider $\varphi^y(x) = x + Df_a^{-1} \left(y - f(x)
    \right)$. Note that 
    \begin{align}
      D \varphi^y_x = & I - Df_a^{-1} Df_x \notag \\ 
      = & Df_a^{-1} (Df_a - Df_x) \leq \norm{Df_a^{-1}} \lambda =
      \frac{1}{2} \label{e50}
    \end{align}
    Then, by the mean value theorem for $x_1, x_2 \in U$, 
    \[ \norm{\varphi^y(x_1) - \varphi^y(x_2)} = \norm{D\varphi^y_{\bar{x}}
      (x_1 - x_2) } \leq \frac{1}{2} \norm{x_1 - x_2}. \].
    It follows that if $\varphi^y(x_1) = x_1$ and $\varphi^y(x_2) =
    x_2$, then $x_1 = x_2$.
    When $\varphi^y(x) = x$, it must be that $y = f(x)$. Thus for each
    $y \in f(U)$, there is at most one $x$ such that $f(x) = y$. That
    is, $f$ is one-to-one on $U$. This proves the first part of the
    theorem and that $f^{-1}$ exists. 
  \end{proof}
\end{frame}

\begin{frame}\begin{proof}
    We now show that $f^{-1}$ is continuously differentiable with the
    stated derivative. Let $y, y+k \in V = f(U)$. Then $\exists x, x+h
    \in U$ such that $y = f(x)$ and $y+k = f(x+h)$. With $\varphi^y$ as
    defined above, we have
  \begin{align*}
    \varphi^y(x+h) - \varphi^y(x) = & h + Df_a^{-1}(f(x) - f(x+h))  \\
    = & h - Df_a^{-1} k
  \end{align*}
  By \ref{e50}, $\norm{h - Df_a^{-1} k} \leq \frac{1}{2} \norm{h}$. It
  follows that $\norm{Df_a^{-1} k} \geq \frac{1}{2} \norm{h}$ and 
  \[ \norm{h} \leq 2 \norm{Df_a^{-1}} \norm{k} = \lambda^{-1}
  \norm{k}. \]
  Importantly as $k \to 0$, we also have $h \to 0$. Now, 
  \begin{align*}
    \frac{\norm{f^{-1} (y+k) - f^{-1}(y) - Df_x^{-1} k }}{\norm{k}}
    = & \frac{\norm{-Df_x^{-1}(f(x+h) - f(x) - Df_x h)}}{\norm{k}}
    \\
    \leq & \norm{Df_x}^{-1}\lambda \frac{\norm{f(x+h) - f(x) - Df_x
        h}}{\norm{h}} \\
    \lim_{k \to 0} \frac{\norm{f^{-1} (y+k) - f^{-1}(y) - Df_x^{-1} k
      }}{\norm{k}} \leq & \lim_{k \to 0} \norm{Df_x}^{-1}\lambda
    \frac{\norm{f(x+h) - f(x) - Df_x h}}{\norm{h}} = 0
  \end{align*}
  Finally, since $Df_x$ is continuous, so is $(Df_{f^{-1}(y)})^{-1}$,
  which is the derivative of $f^{-1}$.  
\end{proof}
\end{frame}

\subsection{Implicit functions}

\begin{frame}\frametitle{Implicit functions}
  \begin{itemize}
  \item Sometimes can write $f(x) = y$ when want to solve for $x$ 
    \begin{itemize}
    \item Inverse function theorem
    \end{itemize}
  \item Other times, only have $f(x,y) = c$ and want to solve for $x$
    \begin{itemize}
    \item Implicit function theorem
    \end{itemize}
  \item Same idea: take first order expansion of $f$ and work with
    linear system whose coefficients are the derivatives of $f$. 
  \end{itemize}
\end{frame}

\begin{frame}
  TO BE CONTINUED
\end{frame}

%30.4 and 30.5: optimization; cover later



\end{document}
